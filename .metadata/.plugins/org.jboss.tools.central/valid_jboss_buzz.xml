<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>An introduction to JavaScript SDK for CloudEvents</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/QyzqZRtXZYY/" /><category term="Event-Driven" /><category term="JavaScript" /><category term="Kubernetes" /><category term="Node.js" /><category term="CloudEvents" /><category term="serverless functions" /><category term="Typescript" /><author><name>Lucas Holmquist</name></author><id>https://developers.redhat.com/blog/?p=810797</id><updated>2021-03-09T08:00:50Z</updated><published>2021-03-09T08:00:50Z</published><content type="html">&lt;p&gt;In today&amp;#8217;s world of &lt;a href="https://developers.redhat.com/topics/serverless-architecture"&gt;serverless&lt;/a&gt; functions and &lt;a href="https://developers.redhat.com/topics/microservices"&gt;microservices&lt;/a&gt;, events are everywhere. The problem is that they are described differently depending on the producer technology you use.&lt;/p&gt; &lt;p&gt;Without a common standard, the burden is on developers to constantly relearn how to consume events. Not having a standard also makes it more difficult for authors of libraries and tooling to deliver event data across environments like SDKs. Recently, a new project was created to help with this effort.&lt;/p&gt; &lt;p&gt;&lt;a target="_blank" rel="nofollow" href="https://cloudevents.io/"&gt;CloudEvents&lt;/a&gt; is a specification for describing event data in common formats to provide interoperability across services, platforms, and systems. In fact, Red Hat OpenShift Serverless Functions uses CloudEvents. For more information about this new developer feature, see &lt;a target="_blank" rel="nofollow" href="/blog/2021/01/04/create-your-first-serverless-function-with-red-hat-openshift-serverless-functions/"&gt;&lt;em&gt;Create your first serverless function with Red Hat OpenShift Serverless Functions&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;The CloudEvents specification&lt;/h2&gt; &lt;p&gt;The specification&amp;#8217;s goal isn’t to create yet another event format and try to force everyone to use it. Rather, we want to define common metadata for events and establish where this metadata should appear in the message being sent.&lt;/p&gt; &lt;p&gt;It is a simple spec with simple goals. In fact, a CloudEvent requires only four pieces of metadata:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;type&lt;/code&gt; describes what kind of event this might be (e.g., a “create” event).&lt;/li&gt; &lt;li&gt;&lt;code&gt;specversion&lt;/code&gt; denotes the version of the spec used to create the CloudEvent.&lt;/li&gt; &lt;li&gt;&lt;code&gt;source&lt;/code&gt; describes where the event came from.&lt;/li&gt; &lt;li&gt;&lt;code&gt;id&lt;/code&gt; is a unique identifier that is useful for de-duping.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;There are other useful fields, like &lt;code&gt;subject&lt;/code&gt;, which when combined with &lt;code&gt;source&lt;/code&gt; can add a little more context to where the event originated.&lt;/p&gt; &lt;p&gt;As I mentioned, the CloudEvents specification is only concerned with the common metadata listed above, and the location where this metadata is placed when sending the event.&lt;/p&gt; &lt;p&gt;Currently, there are two event formats: Binary, which is the preferred format, and structured. Binary is recommended because it is additive. That is, the binary format only adds some headers to the HTTP request. If there is a middleware that doesn’t understand CloudEvents, it won’t break anything, but if that system is updated to support CloudEvents, it starts working.&lt;/p&gt; &lt;p&gt;Structured formats are for those who don’t have any format currently defined and are looking for guidance on how things should be structured.&lt;/p&gt; &lt;p&gt;Here is a quick example of what those two event formats might look like in raw HTTP:&lt;/p&gt; &lt;pre&gt;// Binary Post /event HTTP/1.0 Host: example.com Content-Type: application/json ce-specversion: 1.0 ce-type: com.nodeshift.create ce-source: nodeshift.dev ce-id: 123456 { "action": "createThing", "item": "2187" } // Structured Post /event HTTP/1.0 Host: example.com Content-Type: application/cloudevents+json { "specversion": "1.0" "type": "com.nodeshift.create" "source": "nodeshift.dev" "id": "123456" "data": { "action": "createThing", "item": "2187" } } &lt;/pre&gt; &lt;h2&gt;JavaScript SDK for CloudEvents&lt;/h2&gt; &lt;p&gt;Of course, we don’t want to have to format these events manually. That is where the &lt;a target="_blank" rel="nofollow" href="https://www.npmjs.com/package/cloudevents"&gt;JavaScript SDK for CloudEvents&lt;/a&gt; comes in. There are three main goals that an SDK should accomplish:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Compose an event.&lt;/li&gt; &lt;li&gt;Encode an event for sending.&lt;/li&gt; &lt;li&gt;Decode an incoming event.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Installing the JavaScript SDK is like using any other Node module:&lt;/p&gt; &lt;pre&gt;$ npm install cloudevents &lt;/pre&gt; &lt;p&gt;Now that we’ve seen what a CloudEvent is and how it is useful let&amp;#8217;s take a look at an example.&lt;/p&gt; &lt;h2&gt;Create a new CloudEvent&lt;/h2&gt; &lt;p&gt;First, we are going to create a new CloudEvent object:&lt;/p&gt; &lt;pre&gt;const { CloudEvent } = require('cloudevents'); // Create a new CloudEvent const ce = new CloudEvent({ type: 'com.cloudevent.fun', source: 'fun-with-cloud-events', data: { key: 'DATA' } }); &lt;/pre&gt; &lt;p&gt;If we log this out with the object&amp;#8217;s built-in &lt;code&gt;toJSON&lt;/code&gt; method, we might see something like this:&lt;/p&gt; &lt;pre&gt;console.log(ce.toJSON()); { id: '...', type: 'com.cloudevent.fun', source: 'fun-with-cloud-events', specversion: '1.0', time: '...', data: { key: 'DATA' } } &lt;/pre&gt; &lt;h3&gt;Sending the message&lt;/h3&gt; &lt;p&gt;Next, let&amp;#8217;s look at how to send this over HTTP using the binary format.&lt;/p&gt; &lt;p&gt;First, we need to create our message in the binary format, which you can do easily with the &lt;code&gt;HTTP.binary&lt;/code&gt; method. We will use the CloudEvent from the previous example:&lt;/p&gt; &lt;pre&gt; const message = HTTP.binary(ce); //const message = HTTP.structured(ce); // Showing just for completeness &lt;/pre&gt; &lt;p&gt;Again, if we log this out, it might look something like this:&lt;/p&gt; &lt;pre&gt; headers: { 'content-type': 'application/json;', 'ce-id': '...', 'ce-type': 'com.cloudevent.fun', 'ce-source': 'fun-with-cloud-events', 'ce-specversion': '1.0', 'ce-time': '...' }, body: { key: 'DATA' } } &lt;/pre&gt; &lt;p&gt;Now that the message has been formatted properly, we can send it by using a library like &lt;a target="_blank" rel="nofollow" href="https://github.com/axios/axios"&gt;Axios&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Note that the CloudEvents SDK doesn’t handle sending messages; it only handles formatting the message headers and message body. This allows you to use any HTTP library you want to send the message.&lt;/p&gt; &lt;pre&gt;const axios = require('axios') axios({ method: 'post', url: 'http://localhost:3000/cloudeventy', data: message.body, headers: message.headers }).then((response) =&amp;#62; { console.log(response.data); }); &lt;/pre&gt; &lt;p&gt;We are sending a POST request to the “cloudevent-y” REST endpoint. In this example, I have used a simple Express.js application, but you can use any framework you like.&lt;/p&gt; &lt;h3&gt;Receiving the message&lt;/h3&gt; &lt;p&gt;Once we have the message, we can use the &lt;code&gt;HTTP.toEvent&lt;/code&gt; method to convert it back into a CloudEvent object.&lt;/p&gt; &lt;pre&gt;const express = require('express'); const { HTTP } = require('cloudevents'); const app = express(); app.post('/cloudeventy', (req, res) =&amp;#62; { const ce = HTTP.toEvent({ headers: req.headers, body: req.body }); console.log(ce.toJSON()); res.send({key: 'Event Received'}); }); &lt;/pre&gt; &lt;p&gt;Again, the log output looks similar to what we saw when we output the CloudEvent object:&lt;/p&gt; &lt;pre&gt;{ id: '...', type: 'com.cloudevent.fun', source: 'fun-with-cloud-events', specversion: '1.0', time: '...', data: { key: 'DATA' } } &lt;/pre&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;To learn more about the JavaScript SDK for CloudEvents, &lt;a target="_blank" rel="nofollow" href="https://github.com/cloudevents/sdk-javascript"&gt;check out the GitHub project&lt;/a&gt;. For more information about the history, development, and design rationale behind the specification, see the &lt;a href="https://github.com/cloudevents/spec/blob/master/primer.md" target="_blank" target="_blank" rel="nofollow" noreferrer"&gt;CloudEvents Primer&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F09%2Fan-introduction-to-javascript-sdk-for-cloudevents%2F&amp;#38;linkname=An%20introduction%20to%20JavaScript%20SDK%20for%20CloudEvents" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F09%2Fan-introduction-to-javascript-sdk-for-cloudevents%2F&amp;#38;linkname=An%20introduction%20to%20JavaScript%20SDK%20for%20CloudEvents" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F09%2Fan-introduction-to-javascript-sdk-for-cloudevents%2F&amp;#38;linkname=An%20introduction%20to%20JavaScript%20SDK%20for%20CloudEvents" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F09%2Fan-introduction-to-javascript-sdk-for-cloudevents%2F&amp;#38;linkname=An%20introduction%20to%20JavaScript%20SDK%20for%20CloudEvents" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F09%2Fan-introduction-to-javascript-sdk-for-cloudevents%2F&amp;#38;linkname=An%20introduction%20to%20JavaScript%20SDK%20for%20CloudEvents" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F09%2Fan-introduction-to-javascript-sdk-for-cloudevents%2F&amp;#38;linkname=An%20introduction%20to%20JavaScript%20SDK%20for%20CloudEvents" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F09%2Fan-introduction-to-javascript-sdk-for-cloudevents%2F&amp;#38;linkname=An%20introduction%20to%20JavaScript%20SDK%20for%20CloudEvents" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F09%2Fan-introduction-to-javascript-sdk-for-cloudevents%2F&amp;#038;title=An%20introduction%20to%20JavaScript%20SDK%20for%20CloudEvents" data-a2a-url="https://developers.redhat.com/blog/2021/03/09/an-introduction-to-javascript-sdk-for-cloudevents/" data-a2a-title="An introduction to JavaScript SDK for CloudEvents"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/03/09/an-introduction-to-javascript-sdk-for-cloudevents/"&gt;An introduction to JavaScript SDK for CloudEvents&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/QyzqZRtXZYY" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;In today&amp;#8217;s world of serverless functions and microservices, events are everywhere. The problem is that they are described differently depending on the producer technology you use. Without a common standard, the burden is on developers to constantly relearn how to consume events. Not having a standard also makes it more difficult for authors of libraries [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/03/09/an-introduction-to-javascript-sdk-for-cloudevents/"&gt;An introduction to JavaScript SDK for CloudEvents&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2021/03/09/an-introduction-to-javascript-sdk-for-cloudevents/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">810797</post-id><dc:creator>Lucas Holmquist</dc:creator><dc:date>2021-03-09T08:00:50Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/03/09/an-introduction-to-javascript-sdk-for-cloudevents/</feedburner:origLink></entry><entry><title>Deploying Node.js applications to Kubernetes with Nodeshift and Minikube</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/fAqJZg6t3vY/" /><category term="Developer Tools" /><category term="JavaScript" /><category term="Kubernetes" /><category term="Node.js" /><category term="minikube" /><category term="nodeshift" /><category term="openshift" /><category term="S2I" /><author><name>Lucas Holmquist</name></author><id>https://developers.redhat.com/blog/?p=865157</id><updated>2021-03-09T08:00:39Z</updated><published>2021-03-09T08:00:39Z</published><content type="html">&lt;p&gt;In a &lt;a target="_blank" rel="nofollow" href="/blog/2019/08/30/easily-deploy-node-js-applications-to-red-hat-openshift-using-nodeshift/"&gt;previous article&lt;/a&gt;, I showed how easy it was to deploy a &lt;a target="_blank" rel="nofollow" href="/topics/nodejs"&gt;Node.js&lt;/a&gt; application during development to &lt;a target="_blank" rel="nofollow" href="/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt; using the Nodeshift command-line interface (CLI). In this article, we will take a look at using Nodeshift to deploy Node.js applications to vanilla &lt;a target="_blank" rel="nofollow" href="/topics/kubernetes/"&gt;Kubernetes&lt;/a&gt;—specifically, with Minikube.&lt;/p&gt; &lt;h2&gt;Getting started&lt;/h2&gt; &lt;p&gt;If you want to follow along with this tutorial, you will need to run Minikube. I won&amp;#8217;t cover the setup process, but &lt;a target="_blank" rel="nofollow" href="https://minikube.sigs.k8s.io/docs/start/"&gt;Minikube&amp;#8217;s documentation&lt;/a&gt; can guide you through it. For the tutorial, I also assume that you have installed &lt;a target="_blank" rel="nofollow" href="https://nodejs.org/en/download"&gt;Node.js and Node Package Manager (npm)&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The code samples we&amp;#8217;ll use are available on &lt;a target="_blank" rel="nofollow" href="https://github.com/nodeshift-starters/basic-node-app-dockerized"&gt;GitHub&lt;/a&gt;. Our example is a very basic Node.js application with a Dockerfile. In fact, it is taken from the &lt;a target="_blank" rel="nofollow" href="https://nodejs.org/en/docs/guides/nodejs-docker-webapp/"&gt;&lt;i&gt;Dockerizing a Node.js web app&lt;/i&gt;&lt;/a&gt; guide on Nodejs.org.&lt;/p&gt; &lt;h2&gt;The Nodeshift CLI&lt;/h2&gt; &lt;p&gt;As the Nodeshift module readme states, Nodeshift is an opinionated command-line application and programmable API that you can use to deploy Node.js applications to &lt;a target="_blank" rel="nofollow" href="/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt;. You can easily run it using the &lt;code&gt;npx&lt;/code&gt; command, and it will create the appropriate YAML files to deploy your application.&lt;/p&gt; &lt;p&gt;Nodeshift is a great tool to use if you are developing against an OpenShift cluster, which uses the Source-to-Image (S2I) workflow. In short, Nodeshift creates an OpenShift &lt;code&gt;BuildConfig&lt;/code&gt;, which calls a Node.js S2I image to build your Node application. In most cases, you can achieve this by running &lt;code&gt;npm install&lt;/code&gt;. The build result is put into an OpenShift &lt;code&gt;ImageStream&lt;/code&gt; that resides in the internal OpenShift container registry. This image is then used to deploy your application.&lt;/p&gt; &lt;p&gt;But what about deploying to a vanilla Kubernetes cluster that doesn’t know anything about BuildConfigs, ImageStreams, or S2I? Well, as of &lt;a href="https://github.com/nodeshift/nodeshift/releases/tag/v7.3.0"&gt;Nodeshift&amp;#8217;s 7.3 release&lt;/a&gt;, you can now deploy your Node.js applications to Minikube.&lt;/p&gt; &lt;h2&gt;Deploying Node.js to Minikube&lt;/h2&gt; &lt;p&gt;Before we look at how Nodeshift works for deploying a Node.js application to Minikube, let’s take a minute for a high-level overview of deploying to Kubernetes.&lt;/p&gt; &lt;p&gt;First, you will create an application container image, which you can do with Docker. Once you have a container image, you&amp;#8217;ll need to push that image to a container registry that your cluster has access to, something like &lt;a target="_blank" rel="nofollow" href="https://hub.docker.com/"&gt;Docker Hub&lt;/a&gt;. Once the image is available, you must then specify that image in your deployment YAML and create a service to expose the application.&lt;/p&gt; &lt;p&gt;This flow starts to be more cumbersome when you start iterating on your code. It isn’t really development-friendly if you need to run a Docker build and push that new image to Docker Hub every time. Not to mention that you also need to update your deployment with the new version of the image to ensure it redeploys.&lt;/p&gt; &lt;p&gt;Nodeshift&amp;#8217;s goal is to make developers&amp;#8217; lives easier when deploying to OpenShift and Kubernetes. Let&amp;#8217;s see how Nodeshift helps with each of those unwieldy steps.&lt;/p&gt; &lt;h2&gt;Minikube&amp;#8217;s internal Docker server&lt;/h2&gt; &lt;p&gt;A major difference between OpenShift and Kubernetes is that there is no easy way to run S2I builds on plain Kubernetes. We also don’t want to run a Docker build and push to Docker Hub every time we change our code. Fortunately, Minikube gives us an alternative.&lt;/p&gt; &lt;p&gt;Minikube has its own internal Docker server that we can connect to using the &lt;a target="_blank" rel="nofollow" href="https://docs.docker.com/engine/api/v1.41/#"&gt;Docker Engine API&lt;/a&gt;. We can use this server to run our Docker build in the environment, which means that we don’t have to push the image to an external resource like Docker Hub. We can then use this image in our deployment.&lt;/p&gt; &lt;p&gt;To get access to the internal Docker server, Minikube has a command to export some environment variables to add to your terminal shell. This command is &lt;code&gt;minikube docker-env&lt;/code&gt;, which might output something like this:&lt;/p&gt; &lt;pre&gt;export DOCKER_TLS_VERIFY="1" export DOCKER_HOST="tcp://192.168.39.12:2376" export DOCKER_CERT_PATH="/home/lucasholmquist/.minikube/certs" export MINIKUBE_ACTIVE_DOCKERD="minikube" # To point your shell to minikube's docker-daemon, run: # eval $(minikube -p minikube docker-env) &lt;/pre&gt; &lt;h2&gt;Making it easier with Nodeshift&lt;/h2&gt; &lt;p&gt;Nodeshift abstracts the details we don’t really care about so we can focus on our applications. In this case, we don’t want to think about how to connect to Minikube&amp;#8217;s internal server or how to run Docker commands by hand, and we don’t want to think about updating our deployment YAML every time we build a new image to redeploy it.&lt;/p&gt; &lt;p&gt;Using the Nodeshift CLI with the &lt;code&gt;--kube&lt;/code&gt; flag simplifies those tasks. Let’s see how it works using our &lt;a target="_blank" rel="nofollow" href="https://github.com/nodeshift-starters/basic-node-app-dockerized"&gt;example application&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;We will use &lt;code&gt;npx&lt;/code&gt; to deploy the Node.js application to Minikube, so we don’t need to install anything globally. Run it like this in the example directory:&lt;/p&gt; &lt;pre&gt;$ npx nodeshift --kube &lt;/pre&gt; &lt;p&gt;Nodeshift creates a service and deployment by default if none are provided. Also, note that the type of service it creates is a &lt;code&gt;LoadBalancer&lt;/code&gt;, which allows us to expose our application without using ingress.&lt;/p&gt; &lt;p&gt;The Nodeshift CLI runs the same &lt;code&gt;goals&lt;/code&gt; for a Kubernetes deploy as it does for an OpenShift deploy. The key difference comes during the &lt;code&gt;build&lt;/code&gt; phase. Instead of creating an OpenShift &lt;code&gt;BuildConfig&lt;/code&gt; and running an S2I process on the cluster, Nodeshift uses the &lt;a target="_blank" rel="nofollow" href="https://www.npmjs.com/package/dockerode"&gt;dockerode&lt;/a&gt; module to connect to Minikube&amp;#8217;s internal Docker server and run a build using the provided Dockerfile. The built image is now in that internal registry, ready to be deployed by the deployment YAML that the Nodeshift CLI creates. Nodeshift also adds a randomly-generated number to the deployment&amp;#8217;s metadata, which is then applied during every redeploy. This will trigger Minikube to redeploy the application with the new image.&lt;/p&gt; &lt;p&gt;The following is an example log output:&lt;/p&gt; &lt;pre&gt;~/develop/nodeshift-starters/basic-node-app-dockerized» npx nodeshift --kube 2021-02-09T20:03:18.405Z INFO loading configuration 2021-02-09T20:03:18.452Z INFO Using the kubernetes flag. 2021-02-09T20:03:18.762Z INFO using namespace default at https://192.168.39.12:8443 2021-02-09T20:03:18.763Z WARNING a file property was not found in your package.json, archiving the current directory. 2021-02-09T20:03:18.773Z INFO creating archive of .dockerignore, .gitignore, Dockerfile, README.md, package-lock.json, package.json, server.js 2021-02-09T20:03:18.774Z INFO Building Docker Image 2021-02-09T20:03:18.848Z TRACE {"stream":"Step 1/7 : FROM node:14"} 2021-02-09T20:03:18.848Z TRACE {"stream":"\n"} 2021-02-09T20:03:18.849Z TRACE {"stream":" ---\u003e cb544c4472e9\n"} 2021-02-09T20:03:18.849Z TRACE {"stream":"Step 2/7 : WORKDIR /usr/src/app"} 2021-02-09T20:03:18.849Z TRACE {"stream":"\n"} 2021-02-09T20:03:18.849Z TRACE {"stream":" ---\u003e Using cache\n"} 2021-02-09T20:03:18.849Z TRACE {"stream":" ---\u003e 57c9e3a4e918\n"} 2021-02-09T20:03:18.849Z TRACE {"stream":"Step 3/7 : COPY package*.json ./"} 2021-02-09T20:03:18.850Z TRACE {"stream":"\n"} 2021-02-09T20:03:19.050Z TRACE {"stream":" ---\u003e 742050ca3266\n"} 2021-02-09T20:03:19.050Z TRACE {"stream":"Step 4/7 : RUN npm install"} 2021-02-09T20:03:19.050Z TRACE {"stream":"\n"} 2021-02-09T20:03:19.109Z TRACE {"stream":" ---\u003e Running in f3477d5f2b00\n"} 2021-02-09T20:03:21.739Z TRACE {"stream":"\u001b[91mnpm WARN basic-node-app-dockerized@1.0.0 No description\n\u001b[0m"} 2021-02-09T20:03:21.744Z TRACE {"stream":"\u001b[91mnpm WARN basic-node-app-dockerized@1.0.0 No repository field.\n\u001b[0m"} 2021-02-09T20:03:21.745Z TRACE {"stream":"\u001b[91m\n\u001b[0m"} 2021-02-09T20:03:21.746Z TRACE {"stream":"added 50 packages from 37 contributors and audited 50 packages in 1.387s\n"} 2021-02-09T20:03:21.780Z TRACE {"stream":"found 0 vulnerabilities\n\n"} 2021-02-09T20:03:22.303Z TRACE {"stream":"Removing intermediate container f3477d5f2b00\n"} 2021-02-09T20:03:22.303Z TRACE {"stream":" ---\u003e afb97a82c035\n"} 2021-02-09T20:03:22.303Z TRACE {"stream":"Step 5/7 : COPY . ."} 2021-02-09T20:03:22.303Z TRACE {"stream":"\n"} 2021-02-09T20:03:22.481Z TRACE {"stream":" ---\u003e 1a451003c472\n"} 2021-02-09T20:03:22.481Z TRACE {"stream":"Step 6/7 : EXPOSE 8080"} 2021-02-09T20:03:22.482Z TRACE {"stream":"\n"} 2021-02-09T20:03:22.545Z TRACE {"stream":" ---\u003e Running in a76389d44b59\n"} 2021-02-09T20:03:22.697Z TRACE {"stream":"Removing intermediate container a76389d44b59\n"} 2021-02-09T20:03:22.697Z TRACE {"stream":" ---\u003e 8ee240b7f9ab\n"} 2021-02-09T20:03:22.697Z TRACE {"stream":"Step 7/7 : CMD [ \"node\", \"server.js\" ]"} 2021-02-09T20:03:22.698Z TRACE {"stream":"\n"} 2021-02-09T20:03:22.759Z TRACE {"stream":" ---\u003e Running in 1f7325ab3c64\n"} 2021-02-09T20:03:22.911Z TRACE {"stream":"Removing intermediate container 1f7325ab3c64\n"} 2021-02-09T20:03:22.912Z TRACE {"stream":" ---\u003e d7f5d1e95592\n"} 2021-02-09T20:03:22.912Z TRACE {"aux":{"ID":"sha256:d7f5d1e9559242f767b54b168c36df5c7cbce6ebc7eb1145d7f6292f20e8cda2"}} 2021-02-09T20:03:22.913Z TRACE {"stream":"Successfully built d7f5d1e95592\n"} 2021-02-09T20:03:22.929Z TRACE {"stream":"Successfully tagged basic-node-app-dockerized:latest\n"} 2021-02-09T20:03:22.933Z WARNING No .nodeshift directory 2021-02-09T20:03:22.954Z INFO openshift.yaml and openshift.json written to /home/lucasholmquist/develop/nodeshift-starters/basic-node-app-dockerized/tmp/nodeshift/resource/ 2021-02-09T20:03:22.975Z INFO creating new service basic-node-app-dockerized 2021-02-09T20:03:22.979Z TRACE Deployment Applied 2021-02-09T20:03:23.036Z INFO Application running at: http://192.168.39.12:30076 2021-02-09T20:03:23.036Z INFO complete &lt;/pre&gt; &lt;p&gt;Following the deployment, the Nodeshift CLI also provides the URL where the application is running in the console output. The output might look something like this:&lt;/p&gt; &lt;pre&gt;... INFO Application running at http://192.168.39.12:30769 ... &lt;/pre&gt; &lt;p&gt;Navigating to the URL provided returns &amp;#8220;Hello World.&amp;#8221;&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This article gave a brief overview of the Nodeshift CLI&amp;#8217;s support for deploying to Minikube. In the future, we plan to add more Kubernetes platforms and other developer-friendly features, like possibly having the Nodeshift CLI create a default Dockerfile if there isn’t one.&lt;/p&gt; &lt;p&gt;If you like what you see and want to learn more, check out the &lt;a target="_blank" rel="nofollow" href="https://nodeshift.dev/"&gt;Nodeshift project&lt;/a&gt;. As always, if there are more features you would like to see, &lt;a target="_blank" rel="nofollow" href="https://github.com/nodeshift/nodeshift"&gt;create an issue&lt;/a&gt; over on GitHub. To learn more about what Red Hat is up to on the Node.js front, check out our &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js landing page.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F09%2Fdeploying-node-js-applications-to-kubernetes-with-nodeshift-and-minikube%2F&amp;#38;linkname=Deploying%20Node.js%20applications%20to%20Kubernetes%20with%20Nodeshift%20and%20Minikube" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F09%2Fdeploying-node-js-applications-to-kubernetes-with-nodeshift-and-minikube%2F&amp;#38;linkname=Deploying%20Node.js%20applications%20to%20Kubernetes%20with%20Nodeshift%20and%20Minikube" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F09%2Fdeploying-node-js-applications-to-kubernetes-with-nodeshift-and-minikube%2F&amp;#38;linkname=Deploying%20Node.js%20applications%20to%20Kubernetes%20with%20Nodeshift%20and%20Minikube" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F09%2Fdeploying-node-js-applications-to-kubernetes-with-nodeshift-and-minikube%2F&amp;#38;linkname=Deploying%20Node.js%20applications%20to%20Kubernetes%20with%20Nodeshift%20and%20Minikube" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F09%2Fdeploying-node-js-applications-to-kubernetes-with-nodeshift-and-minikube%2F&amp;#38;linkname=Deploying%20Node.js%20applications%20to%20Kubernetes%20with%20Nodeshift%20and%20Minikube" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F09%2Fdeploying-node-js-applications-to-kubernetes-with-nodeshift-and-minikube%2F&amp;#38;linkname=Deploying%20Node.js%20applications%20to%20Kubernetes%20with%20Nodeshift%20and%20Minikube" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F09%2Fdeploying-node-js-applications-to-kubernetes-with-nodeshift-and-minikube%2F&amp;#38;linkname=Deploying%20Node.js%20applications%20to%20Kubernetes%20with%20Nodeshift%20and%20Minikube" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F09%2Fdeploying-node-js-applications-to-kubernetes-with-nodeshift-and-minikube%2F&amp;#038;title=Deploying%20Node.js%20applications%20to%20Kubernetes%20with%20Nodeshift%20and%20Minikube" data-a2a-url="https://developers.redhat.com/blog/2021/03/09/deploying-node-js-applications-to-kubernetes-with-nodeshift-and-minikube/" data-a2a-title="Deploying Node.js applications to Kubernetes with Nodeshift and Minikube"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/03/09/deploying-node-js-applications-to-kubernetes-with-nodeshift-and-minikube/"&gt;Deploying Node.js applications to Kubernetes with Nodeshift and Minikube&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/fAqJZg6t3vY" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;In a previous article, I showed how easy it was to deploy a Node.js application during development to Red Hat OpenShift using the Nodeshift command-line interface (CLI). In this article, we will take a look at using Nodeshift to deploy Node.js applications to vanilla Kubernetes—specifically, with Minikube. Getting started If you want to follow along [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/03/09/deploying-node-js-applications-to-kubernetes-with-nodeshift-and-minikube/"&gt;Deploying Node.js applications to Kubernetes with Nodeshift and Minikube&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2021/03/09/deploying-node-js-applications-to-kubernetes-with-nodeshift-and-minikube/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">865157</post-id><dc:creator>Lucas Holmquist</dc:creator><dc:date>2021-03-09T08:00:39Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/03/09/deploying-node-js-applications-to-kubernetes-with-nodeshift-and-minikube/</feedburner:origLink></entry><entry><title>A guide to Red Hat OpenShift 4.5 installer-provisioned infrastructure on vSphere</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/V3nPGQGz_F8/" /><category term="Kubernetes" /><category term="Linux" /><category term="Operator" /><category term="container host provisioning" /><category term="installer" /><category term="openshift" /><category term="RHEL" /><category term="vSphere" /><author><name>Nuttee Jirattivongvibul</name></author><id>https://developers.redhat.com/blog/?p=785627</id><updated>2021-03-09T08:00:38Z</updated><published>2021-03-09T08:00:38Z</published><content type="html">&lt;p&gt;With &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift 4&lt;/a&gt;, Red Hat completely re-architected how developers install, upgrade, and manage OpenShift to develop applications on &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt;. Under the hood, the installation process uses the &lt;a target="_blank" rel="nofollow" href="https://github.com/openshift/installer"&gt;OpenShift installer&lt;/a&gt; to automate container host provisioning using &lt;a href="https://developers.redhat.com/topics/linux"&gt;Red Hat Enterprise Linux&lt;/a&gt; (RHEL) CoreOS. It is then easy to initialize the cluster and set up the cloud domain name system (DNS), load balancer, storage, and so on.&lt;/p&gt; &lt;p&gt;Initially, the fully automated OpenShift installation option (called &lt;em&gt;installer-provisioned infrastructure&lt;/em&gt;) was available only for public and private clouds. In &lt;a target="_blank" rel="nofollow" href="https://docs.openshift.com/container-platform/4.5/welcome/index.html"&gt;OpenShift 4.5&lt;/a&gt;, the installer was updated to support installer-provisioned infrastructure on &lt;a target="_blank" rel="nofollow" href="https://www.vmware.com/products/vsphere.html"&gt;VMware vSphere&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;This article is for enterprise IT users and developers who run their workloads on vSphere. I will show you how to bring up your OpenShift clusters in 30 minutes without the pain of needing to do manual tasks each time.&lt;/p&gt; &lt;h2&gt;Prerequisites&lt;/h2&gt; &lt;p&gt;Let&amp;#8217;s walk through the prerequisites for using OpenShift&amp;#8217;s installer-provisioned infrastructure with vSphere. Make sure your development environment is set up as follows:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;To download and run the OpenShift installer binary, you will need a &lt;a href="https://developers.redhat.com/topics/linux"&gt;Linux&lt;/a&gt; virtual machine (VM) or a Linux guest on your laptop. This host must be able to access your vCenter and VM subnet.&lt;/li&gt; &lt;li&gt;You will also need a VMware cluster with the following configuration: &lt;ul&gt; &lt;li&gt;One vCenter instance.&lt;/li&gt; &lt;li&gt;An ESXi cluster with the minimum for provisioning a standard OpenShift cluster.&lt;/li&gt; &lt;li&gt;vSphere 6.5 with hardware version 13 or 6.7 update 2.&lt;/li&gt; &lt;li&gt;800GB storage from the datastore.&lt;/li&gt; &lt;li&gt;18 or more virtual central processing units (vCPUs). The minimum setup is three leaders with four vCPUs per node and three followers with two vCPUs per node. The recommended configuration is four vCPUs on followers, even for lab purposes. You will also need a temporary vCPU for the bootstrap machine.&lt;/li&gt; &lt;li&gt;88GB memory. You will need three leaders with 16GB RAM per node, three followers with 8GB RAM per node, and 16GB temporary RAM for the bootstrap machine.&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;Network configurations&lt;/h3&gt; &lt;p&gt;You will need a network and subnet with Dynamic Host Configuration Protocol (DHCP) enabled with the long-lease time for this pool. As an example, my lab network is VM Network with the IP address of 198.18.1.0/24. The DHCP pool is at 198.18.1.11-200.&lt;/p&gt; &lt;p&gt;For the DNS server IP requirements, you will need a DNS server with two A records. Note that the two DNS A records point to the API and ingress virtual IP addresses. These will point to the OpenShift installer-provisioned cluster load balancer (&lt;a target="_blank" rel="nofollow" href="https://www.haproxy.com/"&gt;HAProxy&lt;/a&gt; with Keepalived run as containers in OpenShift nodes).&lt;/p&gt; &lt;p&gt;Here is how OpenShift vSphere&amp;#8217;s installer-provisioned infrastructure simplifies the load balancer service for the cluster:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;api.&amp;#60;cluster-name&amp;#62;.&amp;#60;base-domain&amp;#62;&lt;/code&gt; with one reserved IP address for the API virtual IP on the same cluster network. As an example, the IP address for &lt;code&gt;api.ocp01.example.com&lt;/code&gt; could be 198.18.1.201.&lt;/li&gt; &lt;li&gt;&lt;code&gt;*.apps.&amp;#60;cluster-name&amp;#62;.&amp;#60;base-domain&amp;#62;&lt;/code&gt; with one reserved IP address for the ingress virtual IP on the same cluster network. As an example, the IP address for &lt;code&gt;api.ocp01.example.com&lt;/code&gt; could be 198.18.1.202.&lt;/li&gt; &lt;li&gt;The DNS test result should look like this: &lt;pre&gt;[root@centos7-tools1 ~]# nslookup api.apps.ocp01.example.com Server: 198.18.133.1 Address: 198.18.133.1#53 Name: api.ocp01.example.com Address: 198.18.1.201 [root@centos7-tools1 ~]# nslookup api.apps.ocp01.example.com Server: 198.18.133.1 Address: 198.18.133.1#53 Name: api.ocp01.example.com Address: 198.18.1.201&lt;/pre&gt; &lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;Account privileges&lt;/h3&gt; &lt;p&gt;You will also need to configure the vCenter account privileges specified in the &lt;a target="_blank" rel="nofollow" href="https://docs.openshift.com/container-platform/4.5/installing/installing_vsphere/installing-vsphere-installer-provisioned.html#installation-vsphere-installer-infra-requirements_installing-vsphere-installer-provisioned"&gt;OpenShift guide to installing a cluster on vSphere&lt;/a&gt;. Please set your account privileges before continuing with this guide.&lt;/p&gt; &lt;p&gt;Finally, you&amp;#8217;ll need a Red Hat account to access &lt;a target="_blank" rel="nofollow" href="https://cloud.redhat.com"&gt;cloud.redhat.com&lt;/a&gt; and retrieve your pull secret for the OpenShift self-supported 60-day trial.&lt;/p&gt; &lt;p&gt;When you are done, the infrastructure preparation should look similar to the diagram in Figure 1.&lt;/p&gt; &lt;div id="attachment_875697" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/02/VMware-IPI-step-1.png"&gt;&lt;img aria-describedby="caption-attachment-875697" class="wp-image-875697" src="https://developers.redhat.com/blog/wp-content/uploads/2021/02/VMware-IPI-step-1.png" alt="Components in the external network and OpenShift cluster network." width="640" height="458" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/02/VMware-IPI-step-1.png 937w, https://developers.redhat.com/blog/wp-content/uploads/2021/02/VMware-IPI-step-1-300x215.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2021/02/VMware-IPI-step-1-768x550.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-875697" class="wp-caption-text"&gt;Figure 1: Set up your development environment for installer-provisioned infrastructure on vSphere.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Once your development environment is set up, we can continue to the next step.&lt;/p&gt; &lt;h2&gt;Set up the installer VM&lt;/h2&gt; &lt;p&gt;You need to perform a few one-time tasks before starting the OpenShift installer. Once you&amp;#8217;ve done these tasks, you will be able to re-use them to deploy as many clusters as you like.&lt;/p&gt; &lt;h3&gt;Generate the SSH private key&lt;/h3&gt; &lt;p&gt;Generate your secure shell (SSH) private and public key if you don&amp;#8217;t have one in your &lt;code&gt;~/.ssh/&lt;/code&gt; directory. You will need the key for OpenShift node access when it is time to perform debugging tasks:&lt;/p&gt; &lt;pre&gt;ssh-keygen -t rsa -b 4096 -N '' -f ~/.ssh/id_rsa&lt;/pre&gt; &lt;h3&gt;Obtain the installation and client binaries&lt;/h3&gt; &lt;p&gt;You can create your free account and go to the &lt;a target="_blank" rel="nofollow" href="https://cloud.redhat.com/openshift/install"&gt;Red Hat Cloud Services Portal—OpenShift Cluster Manager&lt;/a&gt; to obtain the installer and client binaries for your operating system. Do the following from the portal&amp;#8217;s web user interface:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Log in to the &lt;a target="_blank" rel="nofollow" href="https://cloud.redhat.com/openshift/install"&gt;OpenShift Cluster Manager&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Click &lt;strong&gt;Create an OpenShift cluster&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Choose &lt;strong&gt;Red Hat OpenShift Container Platform&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;On &lt;strong&gt;Select an infrastructure provider&lt;/strong&gt;, select &lt;strong&gt;Run on VMware vSphere&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;In the OpenShift installer, select your operating system binary, for example, Linux. Then, click &lt;strong&gt;Download installer&lt;/strong&gt;. If you want the latest release, use this download URL: &lt;a target="_blank" rel="nofollow" href="https://mirror.openshift.com/pub/openshift-v4/clients/ocp/latest/openshift-install-linux.tar.gz"&gt;https://mirror.openshift.com/pub/openshift-v4/clients/ocp/latest/openshift-install-linux.tar.gz&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Click &lt;strong&gt;Download pull secret&lt;/strong&gt; or copy the pull secret and save it to a file.&lt;/li&gt; &lt;li&gt;Click &lt;strong&gt;Download command-line tools&lt;/strong&gt; and select your operating system. Or, you can use this URL for the latest Linux binary: &lt;a target="_blank" rel="nofollow" href="https://mirror.openshift.com/pub/openshift-v4/clients/ocp/latest/openshift-client-linux.tar.gz"&gt;https://mirror.openshift.com/pub/openshift-v4/clients/ocp/latest/openshift-client-linux.tar.gz&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Extract the installation and client programs. For example, run this command: &lt;pre&gt;tar xvf openshift-install-linux.tar.gz tar xvf openshift-client-linux.tar.gz cp oc /usr/local/bin/ &lt;/pre&gt; &lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;Add the vCenter root CA certificates to your system trust&lt;/h3&gt; &lt;p&gt;The OpenShift installer requires access to vCenter&amp;#8217;s API, so the vCenter certificate must be trusted. Download the vCenter&amp;#8217;s root certificate authority (CA) certificates and extract and copy them to your system trust:&lt;/p&gt; &lt;pre&gt;export VCENTER=&lt;b&gt;&amp;#60;your vcenter hostname or IP Address&amp;#62;&lt;/b&gt; wget https://${VCENTER}/certs/download.zip --no-check-certificate unzip download.zip cp certs/lin/* /etc/pki/ca-trust/source/anchors update-ca-trust extract &lt;/pre&gt; &lt;p&gt;Your installer machine is now set up, and you can run the installer as many times as you want.&lt;/p&gt; &lt;h2&gt;Demonstration: Deploying a simple cluster&lt;/h2&gt; &lt;p&gt;For this demonstration, we will deploy a quick, standard cluster that doesn&amp;#8217;t require any customization.&lt;/p&gt; &lt;h3&gt;Run the installer&lt;/h3&gt; &lt;p&gt;The OpenShift installer is a command-line interface that requests your input for the following:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;SSH public key: For example, &lt;code&gt;/root/.ssh/id_rsa.pub&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;Platform: vSphere.&lt;/li&gt; &lt;li&gt;vCenter: Your vCenter hostname, for example, &lt;code&gt;vc1.example.com&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;Username: Your vCenter username, for example, &lt;code&gt;administrator@vsphere.local&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;Password: Your vCenter password.&lt;/li&gt; &lt;li&gt;Network: Select your cluster network with the DHCP you previously set up.&lt;br /&gt; The OpenShift installer will connect to your vCenter and list your network for you to select.&lt;/li&gt; &lt;li&gt;A virtual IP address for the API: This is the IP address that you allocated and mapped to the &lt;code&gt;api.&amp;#60;cluster-name&amp;#62;.&amp;#60;base-domain&amp;#62;&lt;/code&gt; DNS record (for example, 198.18.1.201).&lt;/li&gt; &lt;li&gt;A virtual IP address for ingress: This is the IP address that you allocated and mapped to the &lt;code&gt;*.apps.&amp;#60;cluster-name&amp;#62;.&amp;#60;base-domain&amp;#62;&lt;/code&gt; DNS record (for example, 198.18.1.202).&lt;/li&gt; &lt;li&gt;Base domain: This will be the same as your &lt;code&gt;&amp;#60;base-domain&amp;#62;&lt;/code&gt;, such as &lt;code&gt;example.com&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;Cluster name: This will be the same as your &lt;code&gt;&amp;#60;cluster-name&amp;#62;&lt;/code&gt;, such as &lt;code&gt;ocp01&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;Pull secret: The pull secret that you downloaded or copied from the OpenShift cluster management page.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Here is an example of output from the OpenShift installer&amp;#8217;s installation process:&lt;/p&gt; &lt;pre&gt;export INSTALLATION_DIR=$HOME/ocp01-01 mkdir $INSTALLATION_DIR ./openshift-install create cluster --dir=$INSTALLATION_DIR --log-level=info ? SSH Public Key /root/.ssh/id_rsa.pub ? Platform vsphere ? vCenter vc1.example.com ? Username administrator@vsphere.local ? Password [? for help] ************* INFO Connecting to vCenter vc1.example.com INFO Defaulting to only available datacenter: DC1 INFO Defaulting to only available cluster: DC1-Cluster INFO Defaulting to only available datastore: NFS_Datastore ? Network: VM Network ? Virtual IP Address for API: 198.18.1.201 ? Virtual IP Address for Ingress: 198.18.1.202 ? Base Domain: example.com ? Cluster Name: ocp01 ? Pull Secret [? for help] *************** &lt;/pre&gt; &lt;h3&gt;Provision a bootstrap machine and leader nodes&lt;/h3&gt; &lt;p&gt;The OpenShift installer will now provision a bootstrap machine and three leader nodes. Your API virtual IP and ingress virtual IP will first host on the bootstrap machine for the leader nodes to self-initialize with the ignition and bootstrapping process. The deployment in the bootstrapping stage will look like the diagram in Figure 2.&lt;/p&gt; &lt;div id="attachment_875717" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/02/VMware-IPI-step-2.png"&gt;&lt;img aria-describedby="caption-attachment-875717" class="wp-image-875717" src="https://developers.redhat.com/blog/wp-content/uploads/2021/02/VMware-IPI-step-2.png" alt="A diagram of the bootstrapping deployment." width="640" height="458" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/02/VMware-IPI-step-2.png 937w, https://developers.redhat.com/blog/wp-content/uploads/2021/02/VMware-IPI-step-2-300x215.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2021/02/VMware-IPI-step-2-768x550.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-875717" class="wp-caption-text"&gt;Figure 2: Deployment in the bootstrapping stage.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;When the bootstrapping is complete and all the leader node&amp;#8217;s API servers are up, the OpenShift bootstrap node will be destroyed automatically with the installer. Then, the installer will start provisioning the follower nodes with an &lt;a target="_blank" rel="nofollow" href="https://docs.openshift.com/container-platform/4.5/machine_management/creating-infrastructure-machinesets.html"&gt;OpenShift MachineSet&lt;/a&gt;. For this, you could do a machine auto-scaler or manually increase or decrease the nodes later, using either the OpenShift web console or command-line tools (&lt;code&gt;oc&lt;/code&gt; or &lt;code&gt;kubectl&lt;/code&gt;). The API virtual IP and ingress virtual IP will also be moved to &lt;em&gt;hosted&lt;/em&gt; status on the leader nodes and infrastructure and follower nodes.&lt;/p&gt; &lt;p&gt;Your deployment in the provisioning stage will look like the diagram in Figure 3.&lt;/p&gt; &lt;div id="attachment_875727" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/02/VMware-IPI-step-3.png"&gt;&lt;img aria-describedby="caption-attachment-875727" class="wp-image-875727" src="https://developers.redhat.com/blog/wp-content/uploads/2021/02/VMware-IPI-step-3.png" alt="A diagram of the deployment in the provisioning stage." width="640" height="458" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/02/VMware-IPI-step-3.png 937w, https://developers.redhat.com/blog/wp-content/uploads/2021/02/VMware-IPI-step-3-300x215.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2021/02/VMware-IPI-step-3-768x550.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-875727" class="wp-caption-text"&gt;Figure 3: Deployment in the provisioning stage.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Provisioning the OpenShift cluster normally takes 40 to 60 minutes to complete.&lt;/p&gt; &lt;h2&gt;Post-installation configuration&lt;/h2&gt; &lt;p&gt;You can log into your cluster as a default system user by exporting a &lt;code&gt;kubeconfig&lt;/code&gt; file to &lt;code&gt;KUBECONFIG ENV&lt;/code&gt; vars. This file is created during the installation for a specific cluster and stored in &lt;code&gt;$INSTALLATION_DIR/auth/kubeconfig&lt;/code&gt;. Let&amp;#8217;s go through the post-installation configuration process together.&lt;/p&gt; &lt;p&gt;First, export the &lt;code&gt;kubeadmin&lt;/code&gt; credentials:&lt;/p&gt; &lt;pre&gt;export KUBECONFIG=$INSTALLATION_DIR/auth/kubeconfig&lt;/pre&gt; &lt;p&gt;Next, verify that you can run the &lt;code&gt;oc&lt;/code&gt; command successfully using the exported configuration:&lt;/p&gt; &lt;pre&gt;oc whoami oc get node &lt;/pre&gt; &lt;p&gt;Infrastructure administrators and developers can use the OpenShift console to work with Kubernetes clusters. To access the console for the first time, the installer generates a &lt;code&gt;kubeadmin&lt;/code&gt; credential in the &lt;code&gt;$INSTALLATION_DIR/auth/password&lt;/code&gt; file. Use the &lt;code&gt;oc&lt;/code&gt; command to get the URL for your OpenShift console:&lt;/p&gt; &lt;pre&gt;oc -n openshift-console get route &lt;/pre&gt; &lt;p&gt;Copy the URL and open it in your web browser, and use the initial credentials to log in. For the username, enter &lt;code&gt;kubeadmin&lt;/code&gt;; for the password, use the password from &lt;code&gt;$INSTALLATION_DIR/auth/kubeadmin_password&lt;/code&gt;, as shown in Figure 4.&lt;/p&gt; &lt;div id="attachment_792547" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/09/ocp-login.png"&gt;&lt;img aria-describedby="caption-attachment-792547" class="wp-image-792547 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/09/ocp-login-1024x541.png" alt="The login screen." width="640" height="338" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/09/ocp-login-1024x541.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/ocp-login-300x158.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/ocp-login-768x405.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-792547" class="wp-caption-text"&gt;Figure 4: The OpenShift console OAuth login screen.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Figure 5 shows the overview page in the OpenShift console.&lt;/p&gt; &lt;div id="attachment_792557" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/09/ocp-home-overview.png"&gt;&lt;img aria-describedby="caption-attachment-792557" class="wp-image-792557 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/09/ocp-home-overview-1024x527.png" alt="The overview page shows the cluster details, status, and current utilization." width="640" height="329" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/09/ocp-home-overview-1024x527.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/ocp-home-overview-300x154.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/ocp-home-overview-768x395.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-792557" class="wp-caption-text"&gt;Figure 5: The OpenShift console overview page.&lt;/p&gt;&lt;/div&gt; &lt;h2&gt;Container registry storage&lt;/h2&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/courses/openshift/getting-started"&gt;Red Hat OpenShift Container Platform&lt;/a&gt; uses an internal registry to upgrade clusters and support continuous integration and continuous deployment (&lt;a href="https://developers.redhat.com/topics/ci-cd"&gt;CI/CD&lt;/a&gt;) with containers built within clusters. You will need to set up storage for OpenShift&amp;#8217;s internal registry before you can deploy the demo application.&lt;/p&gt; &lt;p style="padding-left: 40px;"&gt;&lt;strong&gt;Note&lt;/strong&gt;: For our example, we will use ReadWriteOnce (RWO) storage that supports only a single registry instance. For production, Red Hat recommends using the scalable registry that requires ReadWriteMany (RWX) storage.&lt;/p&gt; &lt;h3&gt;Configuring the OpenShift image registry&lt;/h3&gt; &lt;p&gt;You can configure the OpenShift image registry using either the web console or a command-line tool. I&amp;#8217;ll show you how to do this task both ways. If you are new to Kubernetes, you might find the OpenShift console helpful for seeing and understanding your cluster&amp;#8217;s status. On the other hand, using a CLI tool is powerful and gets the tasks done quickly with JSON or YAML declarations.&lt;/p&gt; &lt;h4&gt;Using the OpenShift web console&lt;/h4&gt; &lt;p&gt;The first thing you&amp;#8217;ll do is create a persistent volume claim (PVC) with 100GB capacity from the default storage class (&amp;#8220;thin&amp;#8221;) using the VMware datastore:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Change to the &lt;b&gt;openshift-image-registry&lt;/b&gt; project.&lt;/li&gt; &lt;li&gt;Go to &lt;b&gt;Storage &amp;#62; Persistent Volume Claims&lt;/b&gt; and click &lt;b&gt;Create Persistent Volume Claim&lt;/b&gt;, as shown in Figure 6. &lt;p&gt;&lt;div id="attachment_792637" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/09/ocp-registry-pvc-01-1.png"&gt;&lt;img aria-describedby="caption-attachment-792637" class="wp-image-792637 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/09/ocp-registry-pvc-01-1-1024x546.png" alt="The initial page to create a persistent volume claim." width="640" height="341" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/09/ocp-registry-pvc-01-1-1024x546.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/ocp-registry-pvc-01-1-300x160.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/ocp-registry-pvc-01-1-768x409.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-792637" class="wp-caption-text"&gt;Figure 6: Create a persistent volume claim.&lt;/p&gt;&lt;/div&gt;&lt;/li&gt; &lt;li&gt;Enter the following parameters: &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Storage Class&lt;/strong&gt;: &lt;code&gt;thin&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Persistent Volume Claim Name&lt;/strong&gt;: &lt;code&gt;image-registry-storage&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Access Mode&lt;/strong&gt;: &lt;code&gt;Single User (RWO)&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Size&lt;/strong&gt;: &lt;code&gt;100GiB&lt;/code&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;Then, click &lt;b&gt;Create&lt;/b&gt;, as shown in Figure 7. &lt;p&gt;&lt;div id="attachment_792627" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/09/ocp-registry-pvc-02.png"&gt;&lt;img aria-describedby="caption-attachment-792627" class="wp-image-792627 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/09/ocp-registry-pvc-02-1024x637.png" alt="Configure the PVC, then click Create." width="640" height="398" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/09/ocp-registry-pvc-02-1024x637.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/ocp-registry-pvc-02-300x187.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/ocp-registry-pvc-02-768x478.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-792627" class="wp-caption-text"&gt;Figure 7: Configure and create the persistent volume claim.&lt;/p&gt;&lt;/div&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Next, you will edit the registry configuration to use the &lt;strong&gt;&amp;#60;PVC name&amp;#62;&lt;/strong&gt; you&amp;#8217;ve just created and also update the &lt;code&gt;managementState&lt;/code&gt; and &lt;code&gt;updateStrategy&lt;/code&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Go to &lt;b&gt;Administration &amp;#62; Custom Resource Definitions&lt;/b&gt;, shown in Figure 8. &lt;p&gt;&lt;div id="attachment_792617" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/09/ocp-registry-pvc-01.png"&gt;&lt;img aria-describedby="caption-attachment-792617" class="wp-image-792617 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/09/ocp-registry-pvc-01-1024x546.png" alt="Edit the registry configuration to use the new PVC." width="640" height="341" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/09/ocp-registry-pvc-01-1024x546.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/ocp-registry-pvc-01-300x160.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/ocp-registry-pvc-01-768x409.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-792617" class="wp-caption-text"&gt;Figure 8: Go to &lt;strong&gt;Administration &amp;#62; Custom Resource Definitions&lt;/strong&gt; in the OpenShift web console.&lt;/p&gt;&lt;/div&gt;&lt;/li&gt; &lt;li&gt;Click on the &lt;b&gt;CRD Config&lt;/b&gt; of &lt;code&gt;imageregistry.operator.openshift.io&lt;/code&gt;, shown in Figure 9. &lt;p&gt;&lt;div id="attachment_792607" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/09/ocp-registry-pvc-04.png"&gt;&lt;img aria-describedby="caption-attachment-792607" class="wp-image-792607 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/09/ocp-registry-pvc-04-1024x549.png" alt="Click the 'CRD config' link." width="640" height="343" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/09/ocp-registry-pvc-04-1024x549.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/ocp-registry-pvc-04-300x161.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/ocp-registry-pvc-04-768x412.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-792607" class="wp-caption-text"&gt;Figure 9: Click &lt;strong&gt;CRD Config&lt;/strong&gt;.&lt;/p&gt;&lt;/div&gt;&lt;/li&gt; &lt;li&gt;Change to the &lt;b&gt;Instance&lt;/b&gt; tab and click on the config name &lt;b&gt;cluster&lt;/b&gt;, shown in Figure 10. &lt;p&gt;&lt;div id="attachment_792597" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/09/ocp-registry-pvc-06.png"&gt;&lt;img aria-describedby="caption-attachment-792597" class="wp-image-792597 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/09/ocp-registry-pvc-06-1024x543.png" alt="Click the config name 'cluster.'" width="640" height="339" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/09/ocp-registry-pvc-06-1024x543.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/ocp-registry-pvc-06-300x159.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/ocp-registry-pvc-06-768x407.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-792597" class="wp-caption-text"&gt;Figure 10: Click the config name, &lt;strong&gt;cluster&lt;/strong&gt;.&lt;/p&gt;&lt;/div&gt;&lt;/li&gt; &lt;li&gt;Select the action &lt;b&gt;Edit Config&lt;/b&gt;, shown in Figure 11: &lt;p&gt;&lt;div id="attachment_792587" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/09/ocp-registry-pvc-07.png"&gt;&lt;img aria-describedby="caption-attachment-792587" class="wp-image-792587 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/09/ocp-registry-pvc-07-1024x483.png" alt="Edit the configuration." width="640" height="302" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/09/ocp-registry-pvc-07-1024x483.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/ocp-registry-pvc-07-300x142.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/ocp-registry-pvc-07-768x363.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-792587" class="wp-caption-text"&gt;Figure 11: Select &lt;strong&gt;Edit Config&lt;/strong&gt;.&lt;/p&gt;&lt;/div&gt;&lt;/li&gt; &lt;li&gt;Edit the following parameters and click &lt;b&gt;Save&lt;/b&gt;: &lt;pre&gt;managementState: Managed rolloutStrategy: Recreate storage: pvc: claim: image-registry-storage &lt;/pre&gt; &lt;p&gt;Figure 12 shows the resulting YAML file on the &lt;b&gt;Config Details&lt;/b&gt; page.&lt;/p&gt; &lt;p&gt;&lt;div id="attachment_792577" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/09/ocp-registry-pvc-08.png"&gt;&lt;img aria-describedby="caption-attachment-792577" class="wp-image-792577 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/09/ocp-registry-pvc-08-1024x418.png" alt="A screenshot of the YAML file." width="640" height="261" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/09/ocp-registry-pvc-08-1024x418.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/ocp-registry-pvc-08-300x123.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/ocp-registry-pvc-08-768x314.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-792577" class="wp-caption-text"&gt;Figure 12: The edited configuration file.&lt;/p&gt;&lt;/div&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The Image Registry Operator will now create an image registry for your OpenShift cluster. You can check by selecting &lt;b&gt;Project = openshift-image-registry&lt;/b&gt; and going to &lt;b&gt;Workloads &amp;#62; Pods&lt;/b&gt;. You will see the &lt;code&gt;image-registry&lt;/code&gt; pod is in the process of being created, as shown in Figure 13.&lt;/p&gt; &lt;div id="attachment_792567" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/09/ocp-registry-pvc-09.png"&gt;&lt;img aria-describedby="caption-attachment-792567" class="wp-image-792567 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/09/ocp-registry-pvc-09-1024x520.png" alt="The image-registry pod was started seconds ago." width="640" height="325" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/09/ocp-registry-pvc-09-1024x520.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/ocp-registry-pvc-09-300x152.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/ocp-registry-pvc-09-768x390.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-792567" class="wp-caption-text"&gt;Figure 13: The image-registry pod is being created.&lt;/p&gt;&lt;/div&gt; &lt;h4&gt;Using the OpenShift CLI&lt;/h4&gt; &lt;p&gt;Now, we&amp;#8217;ll perform the same tasks using the OpenShift CLI. Once again, we start by creating a persistent volume claim:&lt;/p&gt; &lt;pre&gt;cat &amp;#60;&amp;#60;EOF &amp;#62;&amp;#62; image-registry-pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: image-registry-storage namespace: openshift-image-registry spec: accessModes: - ReadWriteOnce resources: requests: storage: 100Gi storageClassName: 'thin' EOF oc apply -f image-registry-pvc.yaml &lt;/pre&gt; &lt;p&gt;Next, patch the &lt;code&gt;imageregistry.operator.openshift.io config&lt;/code&gt; &amp;#8220;cluster&amp;#8221;:&lt;/p&gt; &lt;pre&gt;$ oc patch configs.imageregistry.operator.openshift.io cluster --type merge --patch '{"spec":{"managementState":"Managed","rolloutStrategy":"Recreate","storage":{"pvc":{"claim":"image-registry-storage"}}}}'&lt;/pre&gt; &lt;p&gt;That&amp;#8217;s it! Your OpenShift cluster is ready to build and deploy to your container applications.&lt;/p&gt; &lt;h2&gt;Conclusion and next steps&lt;/h2&gt; &lt;p&gt;This article showed you how to use OpenShift&amp;#8217;s installer-provisioned infrastructure to quickly create and configure an enterprise-grade, production-ready Kubernetes cluster with Red Hat OpenShift Container Platform on VMware vSphere. Visit &lt;a target="_blank" rel="nofollow" href="https://learn.openshift.com/"&gt;learn.openshift.com&lt;/a&gt; for free, guided hands-on labs to keep learning how to deploy your applications or learn basic OpenShift operations.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F09%2Fa-guide-to-red-hat-openshift-4-5-installer-provisioned-infrastructure-on-vsphere%2F&amp;#38;linkname=A%20guide%20to%20Red%20Hat%20OpenShift%204.5%20installer-provisioned%20infrastructure%20on%20vSphere" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F09%2Fa-guide-to-red-hat-openshift-4-5-installer-provisioned-infrastructure-on-vsphere%2F&amp;#38;linkname=A%20guide%20to%20Red%20Hat%20OpenShift%204.5%20installer-provisioned%20infrastructure%20on%20vSphere" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F09%2Fa-guide-to-red-hat-openshift-4-5-installer-provisioned-infrastructure-on-vsphere%2F&amp;#38;linkname=A%20guide%20to%20Red%20Hat%20OpenShift%204.5%20installer-provisioned%20infrastructure%20on%20vSphere" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F09%2Fa-guide-to-red-hat-openshift-4-5-installer-provisioned-infrastructure-on-vsphere%2F&amp;#38;linkname=A%20guide%20to%20Red%20Hat%20OpenShift%204.5%20installer-provisioned%20infrastructure%20on%20vSphere" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F09%2Fa-guide-to-red-hat-openshift-4-5-installer-provisioned-infrastructure-on-vsphere%2F&amp;#38;linkname=A%20guide%20to%20Red%20Hat%20OpenShift%204.5%20installer-provisioned%20infrastructure%20on%20vSphere" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F09%2Fa-guide-to-red-hat-openshift-4-5-installer-provisioned-infrastructure-on-vsphere%2F&amp;#38;linkname=A%20guide%20to%20Red%20Hat%20OpenShift%204.5%20installer-provisioned%20infrastructure%20on%20vSphere" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F09%2Fa-guide-to-red-hat-openshift-4-5-installer-provisioned-infrastructure-on-vsphere%2F&amp;#38;linkname=A%20guide%20to%20Red%20Hat%20OpenShift%204.5%20installer-provisioned%20infrastructure%20on%20vSphere" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F09%2Fa-guide-to-red-hat-openshift-4-5-installer-provisioned-infrastructure-on-vsphere%2F&amp;#038;title=A%20guide%20to%20Red%20Hat%20OpenShift%204.5%20installer-provisioned%20infrastructure%20on%20vSphere" data-a2a-url="https://developers.redhat.com/blog/2021/03/09/a-guide-to-red-hat-openshift-4-5-installer-provisioned-infrastructure-on-vsphere/" data-a2a-title="A guide to Red Hat OpenShift 4.5 installer-provisioned infrastructure on vSphere"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/03/09/a-guide-to-red-hat-openshift-4-5-installer-provisioned-infrastructure-on-vsphere/"&gt;A guide to Red Hat OpenShift 4.5 installer-provisioned infrastructure on vSphere&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/V3nPGQGz_F8" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;With Red Hat OpenShift 4, Red Hat completely re-architected how developers install, upgrade, and manage OpenShift to develop applications on Kubernetes. Under the hood, the installation process uses the OpenShift installer to automate container host provisioning using Red Hat Enterprise Linux (RHEL) CoreOS. It is then easy to initialize the cluster and set up the [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/03/09/a-guide-to-red-hat-openshift-4-5-installer-provisioned-infrastructure-on-vsphere/"&gt;A guide to Red Hat OpenShift 4.5 installer-provisioned infrastructure on vSphere&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2021/03/09/a-guide-to-red-hat-openshift-4-5-installer-provisioned-infrastructure-on-vsphere/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">785627</post-id><dc:creator>Nuttee Jirattivongvibul</dc:creator><dc:date>2021-03-09T08:00:38Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/03/09/a-guide-to-red-hat-openshift-4-5-installer-provisioned-infrastructure-on-vsphere/</feedburner:origLink></entry><entry><title>Red Hat Summit Virtual Experience 2021: Register today</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/qxEz7hH963I/" /><category term="Uncategorized" /><category term="Burr Sutter" /><category term="red hat summit" /><category term="Summit 2021" /><author><name>Red Hat Developer</name></author><id>https://developers.redhat.com/blog/?p=878357</id><updated>2021-03-08T08:00:55Z</updated><published>2021-03-08T08:00:55Z</published><content type="html">&lt;p&gt;Automation, application deployment, and how to speed up your journey to the cloud. These and other developer hot topics will take center stage at &lt;a target="_blank" rel="nofollow" href="https://www.redhat.com/en/summit"&gt;Red Hat Summit 2021&lt;/a&gt;. Join thousands of your peers by &lt;a target="_blank" rel="nofollow" href="https://reg.summit.redhat.com/flow/redhat/sum21/regGeneralAttendee/login?intcmp=7013a0000026UTXAA2"&gt;registering&lt;/a&gt; for our all-new, free, two-part virtual Summit experience. Keynote speaker Burr Sutter will be delving deep into developer technologies as we come together to learn, share stories of success and failure, and turn knowledge into action.&lt;/p&gt; &lt;p&gt;We’ve reimagined this year’s Red Hat Summit as a multi-part experience that includes two no-cost virtual components in April and June, followed by a series of small-scale in-person events later in the year.&lt;/p&gt; &lt;h2&gt;Virtual Experience | &lt;a target="_blank" rel="nofollow" href="https://www.addevent.com/event/gw5377211"&gt;April 27-28, 2021&lt;/a&gt;&lt;/h2&gt; &lt;p&gt;Join us online from wherever you are in the world.&lt;/p&gt; &lt;p&gt;What to expect:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Keynotes from Red Hat leaders&lt;/li&gt; &lt;li&gt;Exciting news and announcements&lt;/li&gt; &lt;li&gt;Global customer and partner spotlights&lt;/li&gt; &lt;li&gt;Live demos&lt;/li&gt; &lt;li&gt;Access to Red Hat experts&lt;/li&gt; &lt;li&gt;Games and entertainment&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Virtual Experience | &lt;a target="_blank" rel="nofollow" href="https://www.addevent.com/event/Kx5377253"&gt;June 15-16, 2021&lt;/a&gt;&lt;/h2&gt; &lt;p&gt;Build on what you learned in April in this second installment.&lt;/p&gt; &lt;p&gt;What to expect:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Seven channels of breakout sessions featuring in-depth technical content&lt;/li&gt; &lt;li&gt;Even more access to Red Hat experts&lt;/li&gt; &lt;li&gt;Customer stories and global content&lt;/li&gt; &lt;li&gt;Demos, chat lounges, and community engagement&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a target="_blank" rel="nofollow" href="https://reg.summit.redhat.com/flow/redhat/sum21/regGeneralAttendee/login?intcmp=7013a0000026UTXAA2"&gt;Visit the Red Hat Summit site to secure your spot at both events with one registration&lt;/a&gt;. &lt;a target="_blank" rel="nofollow" href="/summit"&gt;Bookmark this page&lt;/a&gt; for the latest Summit-related developer sessions and on-demand videos.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F08%2Fred-hat-summit-virtual-experience-2021-register-today%2F&amp;#38;linkname=Red%20Hat%20Summit%20Virtual%20Experience%202021%3A%20Register%20today" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F08%2Fred-hat-summit-virtual-experience-2021-register-today%2F&amp;#38;linkname=Red%20Hat%20Summit%20Virtual%20Experience%202021%3A%20Register%20today" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F08%2Fred-hat-summit-virtual-experience-2021-register-today%2F&amp;#38;linkname=Red%20Hat%20Summit%20Virtual%20Experience%202021%3A%20Register%20today" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F08%2Fred-hat-summit-virtual-experience-2021-register-today%2F&amp;#38;linkname=Red%20Hat%20Summit%20Virtual%20Experience%202021%3A%20Register%20today" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F08%2Fred-hat-summit-virtual-experience-2021-register-today%2F&amp;#38;linkname=Red%20Hat%20Summit%20Virtual%20Experience%202021%3A%20Register%20today" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F08%2Fred-hat-summit-virtual-experience-2021-register-today%2F&amp;#38;linkname=Red%20Hat%20Summit%20Virtual%20Experience%202021%3A%20Register%20today" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F08%2Fred-hat-summit-virtual-experience-2021-register-today%2F&amp;#38;linkname=Red%20Hat%20Summit%20Virtual%20Experience%202021%3A%20Register%20today" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F08%2Fred-hat-summit-virtual-experience-2021-register-today%2F&amp;#038;title=Red%20Hat%20Summit%20Virtual%20Experience%202021%3A%20Register%20today" data-a2a-url="https://developers.redhat.com/blog/2021/03/08/red-hat-summit-virtual-experience-2021-register-today/" data-a2a-title="Red Hat Summit Virtual Experience 2021: Register today"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/03/08/red-hat-summit-virtual-experience-2021-register-today/"&gt;Red Hat Summit Virtual Experience 2021: Register today&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/qxEz7hH963I" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;Automation, application deployment, and how to speed up your journey to the cloud. These and other developer hot topics will take center stage at Red Hat Summit 2021. Join thousands of your peers by registering for our all-new, free, two-part virtual Summit experience. Keynote speaker Burr Sutter will be delving deep into developer technologies as [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/03/08/red-hat-summit-virtual-experience-2021-register-today/"&gt;Red Hat Summit Virtual Experience 2021: Register today&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2021/03/08/red-hat-summit-virtual-experience-2021-register-today/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">878357</post-id><dc:creator>Red Hat Developer</dc:creator><dc:date>2021-03-08T08:00:55Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/03/08/red-hat-summit-virtual-experience-2021-register-today/</feedburner:origLink></entry><entry><title>New developer quick starts and more in the Red Hat OpenShift 4.7 web console</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/ihpK80S_v6g/" /><category term="CI/CD" /><category term="Kubernetes" /><category term="Operator" /><category term="Serverless" /><category term="helm charts" /><category term="openshift" /><category term="serverless" /><category term="Tekton" /><author><name>Serena Chechile Nichols</name></author><id>https://developers.redhat.com/blog/?p=873037</id><updated>2021-03-08T08:00:26Z</updated><published>2021-03-08T08:00:26Z</published><content type="html">&lt;p&gt;We are continuing to evolve the developer experience in &lt;a target="_blank" rel="nofollow" href="https://www.openshift.com"&gt;Red Hat OpenShift 4.7&lt;/a&gt;. This article highlights what&amp;#8217;s new for developers in the OpenShift 4.7 web console. Keep reading to learn about exciting changes to the topology view, an improved developer catalog experience, new developer quick starts, user interface support for &lt;a target="_blank" rel="nofollow" href="/courses/middleware/openshift-pipelines"&gt;Red Hat OpenShift Pipelines&lt;/a&gt; and &lt;a target="_blank" rel="nofollow" href="/topics/serverless-architecture"&gt;Red Hat OpenShift Serverless&lt;/a&gt;, and more.&lt;/p&gt; &lt;h2&gt;Quick-add in the topology view&lt;/h2&gt; &lt;p&gt;One of my favorite features in OpenShift 4.7 is the new quick-add option in the web console&amp;#8217;s topology view. You can use this UI control to search for an item from the developer catalog directly from the topology view without changing context. As you type, matches are dynamically shown in a list. You can then click on a match to see a quick overview in the right panel, then click on the call-to-action to install it. The demonstration in Figure 1 shows the new quick-add feature.&lt;/p&gt; &lt;div id="attachment_877827" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/02/quickadd.gif"&gt;&lt;img aria-describedby="caption-attachment-877827" class="wp-image-877827 size-full" src="https://developers.redhat.com/blog/wp-content/uploads/2021/03/rh-openshift-console-4.7-fig1.gif" alt="An animated demonstration of the quick-add feature." width="640" height="348" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-877827" class="wp-caption-text"&gt;Figure 1: The new quick-add feature in the OpenShift web console&amp;#8217;s topology view.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Additionally, the web console now offers persistent storage for user settings so that you can persist layouts in the topology view. We&amp;#8217;ve had many requests for this feature, shown in Figure 2.&lt;/p&gt; &lt;div id="attachment_877837" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/02/topology-layout.gif"&gt;&lt;img aria-describedby="caption-attachment-877837" class="wp-image-877837 size-full" src="https://developers.redhat.com/blog/wp-content/uploads/2021/03/rh-openshift-console-4-7-fig2.gif" alt="A demonstration of persistence in the topology graph layouts." width="640" height="369" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-877837" class="wp-caption-text"&gt;Figure 2: Topology graph layouts are now persisted.&lt;/p&gt;&lt;/div&gt; &lt;h2&gt;New features in the developer catalog&lt;/h2&gt; &lt;p&gt;The developer catalog is a one-stop-shop for developers to get started quickly with OpenShift. We have improved the developer experience in OpenShift 4.7 by creating a consistent experience across catalogs while also offering contextual views for specific catalog types.&lt;/p&gt; &lt;p&gt;When entering the developer catalog, users can now view all content in a single catalog. Several sub-catalogs are available by default: Builder images, Helm charts, Operator-backed services, and samples. Other sub-catalogs are available based on the installed &lt;a target="_blank" rel="nofollow" href="/topics/kubernetes/operators"&gt;Operators&lt;/a&gt;. OpenShift 4.7 has sub-catalogs for event sources and virtual machines, and more are coming in future releases.&lt;/p&gt; &lt;p&gt;When you drill into sub-catalogs, the features and filters exposed are specific to a given catalog type. As an example, did you know that administrators can add multiple Helm chart repositories? The Helm chart catalog exposes charts from multiple repositories and lets you filter by any Helm chart repository.&lt;/p&gt; &lt;p&gt;Finally, we have received many requests to allow administrators to customize the developer catalog experience. In OpenShift 4.7, we&amp;#8217;ve added a customization feature for catalog administrators. To modify the developer catalog&amp;#8217;s available categories, you only need to add a customization section to the console operator resource. You can then use the resulting YAML snippet to add the default categories to start with and edit them from there.&lt;/p&gt; &lt;h2&gt;Developer quick starts&lt;/h2&gt; &lt;p&gt;You can now access developer quick starts from the &lt;b&gt;+Add&lt;/b&gt; page or from the &lt;b&gt;Quick Starts&lt;/b&gt; item in the OpenShift web console&amp;#8217;s &lt;b&gt;Help&lt;/b&gt; menu. The quick-starts catalog, shown in Figure 3, offers a variety of new developer quick starts—try one out!&lt;/p&gt; &lt;div id="attachment_873397" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/02/QuickStartsForTheDev.png"&gt;&lt;img aria-describedby="caption-attachment-873397" class="wp-image-873397 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2021/02/QuickStartsForTheDev-1024x562.png" alt="Tiles represent quick starts in the catalog." width="640" height="351" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/02/QuickStartsForTheDev-1024x562.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2021/02/QuickStartsForTheDev-300x165.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2021/02/QuickStartsForTheDev-768x421.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-873397" class="wp-caption-text"&gt;Figure 3: Developer quick starts in the quick-starts catalog.&lt;/p&gt;&lt;/div&gt; &lt;h2&gt;Tekton pipelines&lt;/h2&gt; &lt;p&gt;The OpenShift 4.7 web console offers a couple of enhancements for Tekton pipelines. For one, you can now easily access your Tekton pipeline metrics, as shown in Figure 4.&lt;/p&gt; &lt;div id="attachment_873377" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/02/PipelineMetrics.png"&gt;&lt;img aria-describedby="caption-attachment-873377" class="wp-image-873377 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2021/02/PipelineMetrics-1024x615.png" alt="A demonstration of viewing Tekton metrics in the console." width="640" height="384" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/02/PipelineMetrics-1024x615.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2021/02/PipelineMetrics-300x180.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2021/02/PipelineMetrics-768x461.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-873377" class="wp-caption-text"&gt;Figure 4: Tekton pipeline metrics.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;We&amp;#8217;ve also enhanced the &lt;b&gt;PipelineRun&lt;/b&gt; details page, as demonstrated in Figure 5.&lt;/p&gt; &lt;div id="attachment_877857" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/02/PLREnhancements.gif"&gt;&lt;img aria-describedby="caption-attachment-877857" class="wp-image-877857 size-full" src="https://developers.redhat.com/blog/wp-content/uploads/2021/03/rh-openshift-console-4-7-fig5.gif" alt="A demonstration of viewing the PipelineRun details page." width="640" height="369" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-877857" class="wp-caption-text"&gt;Figure 5: The improved PipelineRun details page.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;From the &lt;b&gt;Events&lt;/b&gt; tab, you can now easily access events related to the &lt;code&gt;PipelineRun&lt;/code&gt;, including &lt;code&gt;TaskRun&lt;/code&gt; and pod events. You can also download logs from the &lt;b&gt;Logs&lt;/b&gt; tab.&lt;/p&gt; &lt;h2&gt;Serverless&lt;/h2&gt; &lt;p&gt;Web console support for OpenShift Serverless includes the ability to create channels. Once created, brokers and channels are displayed in the topology view. In addition to creating subscriptions and triggers from action menus, you can now drag-and-drop to initiate these actions from the topology view.&lt;/p&gt; &lt;p&gt;We have also enhanced the creation flow for event sources. Event sources are custom resources, and we needed to address scalability issues in this feature, so we’ve changed the user experience to be catalog-based. You can now view event sources along with other objects in the service catalog. Alternatively, you can click on the event source type and drill into a catalog solely focused on event sources. As an example, if you had the &lt;a target="_blank" rel="nofollow" href="/integration"&gt;Red Hat Integration&lt;/a&gt; &lt;a target="_blank" rel="nofollow" href="/topics/camel-k"&gt;Camel K&lt;/a&gt; Operator installed, you would see Camel K connectors in the catalog.&lt;/p&gt; &lt;p&gt;We’ve also updated the administrator perspective for OpenShift Serverless. The web console includes a primary navigation section for OpenShift Serverless, which contains two sub-sections. One sub-section focuses on serving resources, and the other is for eventing. You can navigate to these sections to find details about your OpenShift event sources, brokers, triggers, channels, and subscriptions. These items are also accessible in the developer perspective&amp;#8217;s topology view and on the search page.&lt;/p&gt; &lt;h2&gt;We want your feedback&lt;/h2&gt; &lt;p&gt;Community feedback helps us continually improve the OpenShift developer experience, and we want to hear from you. You can attend our office hours on &lt;a target="_blank" rel="nofollow" href="http://openshift.tv"&gt;Red Hat OpenShift Streaming&lt;/a&gt; or join the &lt;a target="_blank" rel="nofollow" href="https://groups.google.com/forum/#!forum/openshift-dev-users"&gt;OpenShift Developer Experience Google group&lt;/a&gt;. We hope you will share your tips for using the OpenShift web console, get help with what doesn’t work, and shape the future of the OpenShift developer experience. Ready to get started? &lt;a target="_blank" rel="nofollow" href="http://www.openshift.com/try"&gt;Try OpenShift today&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F08%2Fnew-developer-quick-starts-and-more-in-the-red-hat-openshift-4-7-web-console%2F&amp;#38;linkname=New%20developer%20quick%20starts%20and%20more%20in%20the%20Red%20Hat%20OpenShift%204.7%20web%20console" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F08%2Fnew-developer-quick-starts-and-more-in-the-red-hat-openshift-4-7-web-console%2F&amp;#38;linkname=New%20developer%20quick%20starts%20and%20more%20in%20the%20Red%20Hat%20OpenShift%204.7%20web%20console" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F08%2Fnew-developer-quick-starts-and-more-in-the-red-hat-openshift-4-7-web-console%2F&amp;#38;linkname=New%20developer%20quick%20starts%20and%20more%20in%20the%20Red%20Hat%20OpenShift%204.7%20web%20console" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F08%2Fnew-developer-quick-starts-and-more-in-the-red-hat-openshift-4-7-web-console%2F&amp;#38;linkname=New%20developer%20quick%20starts%20and%20more%20in%20the%20Red%20Hat%20OpenShift%204.7%20web%20console" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F08%2Fnew-developer-quick-starts-and-more-in-the-red-hat-openshift-4-7-web-console%2F&amp;#38;linkname=New%20developer%20quick%20starts%20and%20more%20in%20the%20Red%20Hat%20OpenShift%204.7%20web%20console" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F08%2Fnew-developer-quick-starts-and-more-in-the-red-hat-openshift-4-7-web-console%2F&amp;#38;linkname=New%20developer%20quick%20starts%20and%20more%20in%20the%20Red%20Hat%20OpenShift%204.7%20web%20console" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F08%2Fnew-developer-quick-starts-and-more-in-the-red-hat-openshift-4-7-web-console%2F&amp;#38;linkname=New%20developer%20quick%20starts%20and%20more%20in%20the%20Red%20Hat%20OpenShift%204.7%20web%20console" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F08%2Fnew-developer-quick-starts-and-more-in-the-red-hat-openshift-4-7-web-console%2F&amp;#038;title=New%20developer%20quick%20starts%20and%20more%20in%20the%20Red%20Hat%20OpenShift%204.7%20web%20console" data-a2a-url="https://developers.redhat.com/blog/2021/03/08/new-developer-quick-starts-and-more-in-the-red-hat-openshift-4-7-web-console/" data-a2a-title="New developer quick starts and more in the Red Hat OpenShift 4.7 web console"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/03/08/new-developer-quick-starts-and-more-in-the-red-hat-openshift-4-7-web-console/"&gt;New developer quick starts and more in the Red Hat OpenShift 4.7 web console&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/ihpK80S_v6g" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;We are continuing to evolve the developer experience in Red Hat OpenShift 4.7. This article highlights what&amp;#8217;s new for developers in the OpenShift 4.7 web console. Keep reading to learn about exciting changes to the topology view, an improved developer catalog experience, new developer quick starts, user interface support for Red Hat OpenShift Pipelines and [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/03/08/new-developer-quick-starts-and-more-in-the-red-hat-openshift-4-7-web-console/"&gt;New developer quick starts and more in the Red Hat OpenShift 4.7 web console&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2021/03/08/new-developer-quick-starts-and-more-in-the-red-hat-openshift-4-7-web-console/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">873037</post-id><dc:creator>Serena Chechile Nichols</dc:creator><dc:date>2021-03-08T08:00:26Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/03/08/new-developer-quick-starts-and-more-in-the-red-hat-openshift-4-7-web-console/</feedburner:origLink></entry><entry><title>What’s new in Red Hat OpenShift’s Web Terminal Operator 1.2</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/KxWHVnxmaAk/" /><category term="DevOps" /><category term="Kubernetes" /><category term="Linux" /><category term="Operator" /><category term="openshift" /><category term="OpenShift Operator" /><category term="web console" /><category term="Web Terminal Operator" /><author><name>jpinkney</name></author><id>https://developers.redhat.com/blog/?p=872357</id><updated>2021-03-08T08:00:11Z</updated><published>2021-03-08T08:00:11Z</published><content type="html">&lt;p&gt;&amp;#160;&lt;/p&gt; &lt;p&gt;&lt;a target="_blank" rel="nofollow" href="/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt;&amp;#8216;s Web Terminal Operator is a way for users to access a web terminal with common cluster tooling pre-installed. This gives you the power and flexibility to work with your product directly through the OpenShift web console, eliminating the need to have all your tooling installed locally.&lt;/p&gt; &lt;p&gt;This article is an overview of the new features introduced in Web Terminal Operator 1.2. These improvements include allowing cluster administrators to securely access the terminal, more information for users when a terminal has shut down due to inactivity, and a tooling update to align with OpenShift 4.7.&lt;/p&gt; &lt;h2&gt;Easier access to the OpenShift web console&lt;/h2&gt; &lt;p&gt;In Web Terminal Operator 1.2, cluster administrators can access the web terminal directly, as shown in Figure 1.&lt;/p&gt; &lt;div id="attachment_872407" style="width: 650px" class="wp-caption alignnone"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/02/Figure-1.gif"&gt;&lt;img aria-describedby="caption-attachment-872407" class="wp-image-872407" src="https://developers.redhat.com/blog/wp-content/uploads/2021/02/Figure-1.gif" alt="A cluster administrator accessing the web terminal on the OpenShift web console." width="640" height="332" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-872407" class="wp-caption-text"&gt;Figure 1: Opening the web terminal as a cluster administrator.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Now, everyone on your cluster can access their own web terminal, regardless of their permission level. Note that for security reasons, cluster administrators will automatically bypass the namespace picker and have their web terminal created in the &lt;code&gt;openshift-terminal&lt;/code&gt; namespace. Other than that, the functionality remains the same as that of a user without the cluster-admin role.&lt;/p&gt; &lt;h2&gt;Understanding why a terminal has stopped&lt;/h2&gt; &lt;p&gt;To conserve resources, the web terminal automatically shuts down after 15 minutes of inactivity. In Web Terminal Operator 1.2, we’ve added more information to help users understand why a terminal has stopped (see Figure 2).&lt;/p&gt; &lt;div id="attachment_872417" style="width: 650px" class="wp-caption alignnone"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/02/Figure-2.png"&gt;&lt;img aria-describedby="caption-attachment-872417" class="wp-image-872417" src="https://developers.redhat.com/blog/wp-content/uploads/2021/02/Figure-2.png" alt="The web console informing the user that the terminal has closed due to inactivity." width="640" height="333" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/02/Figure-2.png 821w, https://developers.redhat.com/blog/wp-content/uploads/2021/02/Figure-2-300x156.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2021/02/Figure-2-768x399.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-872417" class="wp-caption-text"&gt;Figure 2: The terminal window indicating why a terminal has shut down.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Web Terminal Operator 1.2 also offers an easier way to restart the terminal with the click of a button, as shown in Figure 3.&lt;/p&gt; &lt;div id="attachment_872427" style="width: 649px" class="wp-caption alignnone"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/02/Figure-3.gif"&gt;&lt;img aria-describedby="caption-attachment-872427" class="wp-image-872427" src="https://developers.redhat.com/blog/wp-content/uploads/2021/02/Figure-3.gif" alt="Clicking the Restart terminal button to restart the session." width="639" height="336" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-872427" class="wp-caption-text"&gt;Figure 3: Click &lt;b&gt;Restart terminal&lt;/b&gt; to restart a session.&lt;/p&gt;&lt;/div&gt; &lt;h2&gt;Updated tooling&lt;/h2&gt; &lt;p&gt;We have updated the default binaries in Web Terminal Operator 1.2 to include the latest versions of the built-in command-line tools, as shown in Table 1.&lt;/p&gt; &lt;table align="”center”"&gt; &lt;caption&gt;&lt;b&gt;Table 1: Command-line tools in Web Terminal Operator 1.2&lt;/b&gt;&lt;/caption&gt; &lt;tbody&gt; &lt;tr&gt; &lt;th&gt;Binary&lt;/th&gt; &lt;th&gt;Old version&lt;/th&gt; &lt;th&gt;New version&lt;/th&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;code&gt;oc&lt;/code&gt;&lt;/td&gt; &lt;td&gt;4.6.1&lt;/td&gt; &lt;td&gt;4.7.0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;code&gt;kubectl&lt;/code&gt;&lt;/td&gt; &lt;td&gt;1.19.0&lt;/td&gt; &lt;td&gt;1.20.1&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;code&gt;odo&lt;/code&gt;&lt;/td&gt; &lt;td&gt;2.0.0&lt;/td&gt; &lt;td&gt;2.0.4&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;code&gt;helm&lt;/code&gt;&lt;/td&gt; &lt;td&gt;3.3.4&lt;/td&gt; &lt;td&gt;3.5.0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;code&gt;kn&amp;#60;/code&lt;/code&gt;&lt;/td&gt; &lt;td&gt;0.16.1&lt;/td&gt; &lt;td&gt;0.19.1&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;code&gt;tkn&lt;/code&gt;&lt;/td&gt; &lt;td&gt;0.11.0&lt;/td&gt; &lt;td&gt;0.15.0&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Additional resources&lt;/h2&gt; &lt;p&gt;For a peek into how the Web Terminal Operator works under the hood, see &lt;a target="_blank" rel="nofollow" href="https://www.openshift.com/blog/a-deeper-look-at-the-web-terminal-operator-1"&gt;&lt;i&gt;A deeper look at the Web Terminal Operator&lt;/i&gt;&lt;/a&gt; by Angel Misevski. You can also check out the initial release article by Joshua Wood: &lt;a target="_blank" rel="nofollow" href="/blog/2020/10/01/command-line-cluster-management-with-red-hat-openshifts-new-web-terminal-tech-preview/"&gt;&lt;i&gt;Command-line cluster management with Red Hat OpenShift’s new web terminal&lt;/i&gt;&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F08%2Fwhats-new-in-red-hat-openshifts-web-terminal-operator-1-2%2F&amp;#38;linkname=What%E2%80%99s%20new%20in%20Red%20Hat%20OpenShift%E2%80%99s%20Web%20Terminal%20Operator%201.2" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F08%2Fwhats-new-in-red-hat-openshifts-web-terminal-operator-1-2%2F&amp;#38;linkname=What%E2%80%99s%20new%20in%20Red%20Hat%20OpenShift%E2%80%99s%20Web%20Terminal%20Operator%201.2" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F08%2Fwhats-new-in-red-hat-openshifts-web-terminal-operator-1-2%2F&amp;#38;linkname=What%E2%80%99s%20new%20in%20Red%20Hat%20OpenShift%E2%80%99s%20Web%20Terminal%20Operator%201.2" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F08%2Fwhats-new-in-red-hat-openshifts-web-terminal-operator-1-2%2F&amp;#38;linkname=What%E2%80%99s%20new%20in%20Red%20Hat%20OpenShift%E2%80%99s%20Web%20Terminal%20Operator%201.2" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F08%2Fwhats-new-in-red-hat-openshifts-web-terminal-operator-1-2%2F&amp;#38;linkname=What%E2%80%99s%20new%20in%20Red%20Hat%20OpenShift%E2%80%99s%20Web%20Terminal%20Operator%201.2" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F08%2Fwhats-new-in-red-hat-openshifts-web-terminal-operator-1-2%2F&amp;#38;linkname=What%E2%80%99s%20new%20in%20Red%20Hat%20OpenShift%E2%80%99s%20Web%20Terminal%20Operator%201.2" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F08%2Fwhats-new-in-red-hat-openshifts-web-terminal-operator-1-2%2F&amp;#38;linkname=What%E2%80%99s%20new%20in%20Red%20Hat%20OpenShift%E2%80%99s%20Web%20Terminal%20Operator%201.2" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F08%2Fwhats-new-in-red-hat-openshifts-web-terminal-operator-1-2%2F&amp;#038;title=What%E2%80%99s%20new%20in%20Red%20Hat%20OpenShift%E2%80%99s%20Web%20Terminal%20Operator%201.2" data-a2a-url="https://developers.redhat.com/blog/2021/03/08/whats-new-in-red-hat-openshifts-web-terminal-operator-1-2/" data-a2a-title="What’s new in Red Hat OpenShift’s Web Terminal Operator 1.2"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/03/08/whats-new-in-red-hat-openshifts-web-terminal-operator-1-2/"&gt;What&amp;#8217;s new in Red Hat OpenShift&amp;#8217;s Web Terminal Operator 1.2&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/KxWHVnxmaAk" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;&amp;#160; Red Hat OpenShift&amp;#8216;s Web Terminal Operator is a way for users to access a web terminal with common cluster tooling pre-installed. This gives you the power and flexibility to work with your product directly through the OpenShift web console, eliminating the need to have all your tooling installed locally. This article is an overview [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/03/08/whats-new-in-red-hat-openshifts-web-terminal-operator-1-2/"&gt;What&amp;#8217;s new in Red Hat OpenShift&amp;#8217;s Web Terminal Operator 1.2&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2021/03/08/whats-new-in-red-hat-openshifts-web-terminal-operator-1-2/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">872357</post-id><dc:creator>jpinkney</dc:creator><dc:date>2021-03-08T08:00:11Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/03/08/whats-new-in-red-hat-openshifts-web-terminal-operator-1-2/</feedburner:origLink></entry><entry><title>Introduction to the Node.js reference architecture, Part 1: Overview</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/pilWCtxrdN0/" /><category term="JavaScript" /><category term="Node.js" /><category term="Open source" /><category term="npm" /><category term="reference architecture" /><author><name>Michael Dawson</name></author><id>https://developers.redhat.com/blog/?p=865807</id><updated>2021-03-08T08:00:06Z</updated><published>2021-03-08T08:00:06Z</published><content type="html">&lt;p&gt;Welcome to this new series introducing the &lt;a target="_blank" rel="nofollow" href="https://github.com/nodeshift/nodejs-reference-architecture"&gt;Node.js reference architecture&lt;/a&gt; from Red Hat and IBM. This article is an overview of our reasons for developing the Node.js reference architecture—both what we hope the architecture will offer our developer community and what we &lt;em&gt;do not&lt;/em&gt; intend it to do. Future articles will offer a detailed look at different sections of the reference architecture.&lt;/p&gt; &lt;p&gt;Before we dive into this first article, it&amp;#8217;s important to acknowledge that the &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js&lt;/a&gt; reference architecture is a work in progress. The development team is working through different areas, discussing what we&amp;#8217;ve learned, and distilling that information into concise recommendations and guidance. Given the fast pace of development in the &lt;a href="https://developers.redhat.com/topics/javascript"&gt;JavaScript&lt;/a&gt; ecosystem, the reference architecture might never be “finished.” Instead, we&amp;#8217;ll continue updating it to reflect what we learn through new Node.js production deployments and ongoing experience with our deployments at scale. The reference architecture is meant to reflect our current experience and thinking, which will evolve.&lt;/p&gt; &lt;h2&gt;Why we need a Node.js reference architecture&lt;/h2&gt; &lt;p&gt;The JavaScript ecosystem is fast-moving and vibrant. You only need to look at the &lt;a target="_blank" rel="nofollow" href="http://www.modulecounts.com/"&gt;growth rate of Node Package Manager (npm) modules&lt;/a&gt; to see that. In 2016, there were approximately 250,000 npm packages. In 2018, that number climbed to around 525,000, and in 2020 it was roughly 1.1 million. These numbers represent considerable choice and variety in the JavaScript ecosystem. That is clearly a strength for flourishing innovation and testing new ideas.&lt;/p&gt; &lt;p&gt;On the flip side, the wide variety of options can make choosing among Node.js packages very difficult. For any module, you might find several equally good choices, as well as several potentially very bad choices. Every application has a “secret sauce” that is key to its success. It is imperative to find the best fitting, newest, or most innovative package to use for this area of the application. For the rest of the application, you likely want something that works and for which you can share any experiences or best practices across your organization. In the latter case, having a reference architecture can help teams avoid relearning the same things again and again.&lt;/p&gt; &lt;h2&gt;What the reference architecture is&lt;/h2&gt; &lt;p&gt;Our Node.js teams at Red Hat and IBM can&amp;#8217;t be experts on 1.1 million JavaScript packages in the &lt;code&gt;npm&lt;/code&gt; registry. Similarly, we can&amp;#8217;t be involved in all of the projects to the level that we are involved in the Node.js project. Instead, our experience is based on our broad usage of Node.js. This includes large-scale deployments like the &lt;a target="_blank" rel="nofollow" href="https://developer.ibm.com/languages/node-js/articles/nodejs-weather-company-success-story/"&gt;Weather Company&lt;/a&gt;, as well as the work that our consulting groups do with customers.&lt;/p&gt; &lt;p&gt;If every internal team and customer who asks for help with their Node.js application uses different packages, it will be much harder to help them. The question is, how do we share our knowledge across the organization?&lt;/p&gt; &lt;p&gt;We want to help our internal teams and customers make good choices and deployment decisions. In cases where a team doesn&amp;#8217;t need to use a specific package, we can recommend a package based on the experience we’ve built across Red Hat and IBM. As developers, we can use the Node.js reference architecture to share and collaborate across teams and projects and establish common ground within our deployments.&lt;/p&gt; &lt;h2&gt;What the reference architecture is not&lt;/h2&gt; &lt;p&gt;I have described what we hope to do with the Node.js reference architecture. It is just as important to be clear about what we are &lt;em&gt;not&lt;/em&gt; trying to do.&lt;/p&gt; &lt;p&gt;First, the reference architecture is not an attempt to convince or force developers to use the packages we choose. Deployments are varied, and there will be good reasons to use specific modules in different circumstances.&lt;/p&gt; &lt;p&gt;Second, we do not claim that our recommendations are better than the alternatives. As I noted, you will often find several equally good packages or approaches available in the JavaScript ecosystem. Our recommendations favor what the Red Hat and IBM teams have used successfully and the technologies we are familiar with. We are not attempting to steer anyone to the “best” choice but instead to a “good” choice. Having a reference architecture maximizes the likelihood of leveraging lessons already learned and having common ground so that we can help each other.&lt;/p&gt; &lt;h2&gt;About this series&lt;/h2&gt; &lt;p&gt;The Node.js development team is having interesting discussions as we work through each section of the reference architecture. At the same time, we are trying to keep the reference architecture&amp;#8217;s content concise and to the point. As I&amp;#8217;ve mentioned, the goal is to provide good choices for the application&amp;#8217;s general architecture so that developers can focus on the application&amp;#8217;s &amp;#8220;secret sauce.&amp;#8221; In most cases, developers using the reference architecture will want to know what package or technology to use and how. As a result, the reference architecture won&amp;#8217;t include much about the interesting background and discussions that led to our decisions.&lt;/p&gt; &lt;p&gt;This series &lt;em&gt;will&lt;/em&gt; share the viewpoints gained from our internal discussions. As we work through each section of the reference architecture, we&amp;#8217;ll use this series to offer additional references and an opportunity to dive into more detail on related topics. I think you’ll find the varied experience of developers across the Node.js team gets you thinking. I learn something from every section we go through, and I hope you will, too.&lt;/p&gt; &lt;h2&gt;What’s next?&lt;/h2&gt; &lt;p&gt;We plan to cover new topics regularly as part of this series. While you wait for the next installment, we invite you to visit the &lt;a target="_blank" rel="nofollow" href="https://github.com/nodeshift/nodejs-reference-architecture"&gt;Node.js reference architecture repository&lt;/a&gt; on GitHub. You&amp;#8217;ll be able to see the work we&amp;#8217;ve already done and the kinds of topics you can look forward to from this series. To learn more about what Red Hat is up to on the Node.js front, check out our &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js landing page&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F08%2Fintroduction-to-the-node-js-reference-architecture-part-1-overview%2F&amp;#38;linkname=Introduction%20to%20the%20Node.js%20reference%20architecture%2C%20Part%201%3A%20Overview" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F08%2Fintroduction-to-the-node-js-reference-architecture-part-1-overview%2F&amp;#38;linkname=Introduction%20to%20the%20Node.js%20reference%20architecture%2C%20Part%201%3A%20Overview" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F08%2Fintroduction-to-the-node-js-reference-architecture-part-1-overview%2F&amp;#38;linkname=Introduction%20to%20the%20Node.js%20reference%20architecture%2C%20Part%201%3A%20Overview" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F08%2Fintroduction-to-the-node-js-reference-architecture-part-1-overview%2F&amp;#38;linkname=Introduction%20to%20the%20Node.js%20reference%20architecture%2C%20Part%201%3A%20Overview" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F08%2Fintroduction-to-the-node-js-reference-architecture-part-1-overview%2F&amp;#38;linkname=Introduction%20to%20the%20Node.js%20reference%20architecture%2C%20Part%201%3A%20Overview" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F08%2Fintroduction-to-the-node-js-reference-architecture-part-1-overview%2F&amp;#38;linkname=Introduction%20to%20the%20Node.js%20reference%20architecture%2C%20Part%201%3A%20Overview" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F08%2Fintroduction-to-the-node-js-reference-architecture-part-1-overview%2F&amp;#38;linkname=Introduction%20to%20the%20Node.js%20reference%20architecture%2C%20Part%201%3A%20Overview" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F08%2Fintroduction-to-the-node-js-reference-architecture-part-1-overview%2F&amp;#038;title=Introduction%20to%20the%20Node.js%20reference%20architecture%2C%20Part%201%3A%20Overview" data-a2a-url="https://developers.redhat.com/blog/2021/03/08/introduction-to-the-node-js-reference-architecture-part-1-overview/" data-a2a-title="Introduction to the Node.js reference architecture, Part 1: Overview"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/03/08/introduction-to-the-node-js-reference-architecture-part-1-overview/"&gt;Introduction to the Node.js reference architecture, Part 1: Overview&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/pilWCtxrdN0" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;Welcome to this new series introducing the Node.js reference architecture from Red Hat and IBM. This article is an overview of our reasons for developing the Node.js reference architecture—both what we hope the architecture will offer our developer community and what we do not intend it to do. Future articles will offer a detailed look [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/03/08/introduction-to-the-node-js-reference-architecture-part-1-overview/"&gt;Introduction to the Node.js reference architecture, Part 1: Overview&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2021/03/08/introduction-to-the-node-js-reference-architecture-part-1-overview/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">865807</post-id><dc:creator>Michael Dawson</dc:creator><dc:date>2021-03-08T08:00:06Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/03/08/introduction-to-the-node-js-reference-architecture-part-1-overview/</feedburner:origLink></entry><entry><title type="html">Supply chain integration - An architectural introduction</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/RglmmpBvDao/supply-chain-integration-an-architectural-introduction.html" /><author><name>Eric D. Schabell</name></author><id>http://feedproxy.google.com/~r/schabell/jboss/~3/pvxTT3ZYk8A/supply-chain-integration-an-architectural-introduction.html</id><updated>2021-03-08T06:00:00Z</updated><content type="html">Part 1 - An architectural introduction If you've been following my writing over the last few years, you've grown accustomed to my sharing of various architecture blueprints. They're focusing on presenting access to ways of mapping successful implementations for specific use cases. It's an interesting challenge in our mission of creating of architectural content based on common customer adoption patterns. That's very different from most of the traditional marketing activities usually associated with generating content for the sole purpose of positioning products for solutions. When you're basing the content on actual execution in solution delivery, you're cutting out the chuff.  What's that mean? It means that it's going to provide you with a way to implement a solution using open source technologies by focusing on the integrations, structures and interactions that actually have been proven to work. What's not included are any vendor promises that you'll find in normal marketing content. Those promised that when it gets down to implementation crunch time, might not fully deliver on their promises. Enter the term Architectural Blueprint.  Let's look at these blueprints, how their created and what value they provide for your solution designs. THE PROCESS The first step is to decide the use case to start with, which in my case had to be linked to a higher level theme that becomes the leading focus. This higher level theme is not quite boiling the ocean, but it's so broad that it's going to require some division in to smaller parts. In this case we've aligned with the higher level theme being 'Retail' use cases, a vertical focus. This breaks down into the following use cases and in no particular order: * * * Point of sale * Headless eCommerce * Store health and safety * Real-time stock control * Retail data framework The case I'm tackling here is focused on supply chain integration. This use case we've defined as the following: Streamlining integration between different elements of a retail supply chain for on-premise, cloud, and other third-party interactions. The approach taken is to research our existing customers that have implemented solutions in this space, collect their public facing content, research the internal implementation documentation collections from their successful engagements, and where necessary reach out to the field resources involved.  To get an idea of what these blueprints look like, we refer you to the series previously discussed here: * * * * Now on to the task at hand. WHAT'S NEXT The resulting content for this project targets the following three items. * A slide deck of the architectural blueprint for use telling the portfolio solution story. * Generic architectural diagrams providing the general details for the portfolio solution. * A write-up of the portfolio solution in a series that can be used for a customer solution brief. An overview of this series on business optimisation portfolio architecture blueprint: 1. 2. Common architectural elements 3. Example of supply chain integration Catch up on any past articles you missed by following any published links above. Next in this series, taking a look at the generic common architectural elements for the supply chain integration architecture. (Article co-authored by , Chief Architect Retail, Red Hat)&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/RglmmpBvDao" height="1" width="1" alt=""/&gt;</content><dc:creator>Eric D. Schabell</dc:creator><feedburner:origLink>http://feedproxy.google.com/~r/schabell/jboss/~3/pvxTT3ZYk8A/supply-chain-integration-an-architectural-introduction.html</feedburner:origLink></entry><entry><title type="html">Trying DB2 in Kubernetes for developers</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/qPr-LeKzOYU/" /><author><name /></author><id>https://blog.ramon-gordillo.dev/2021/03/trying-db2-in-kubernetes-for-developers/</id><updated>2021-03-08T00:00:00Z</updated><content type="html">Recently I have been asked a lot of similar questions like “does it make sense to invest in deploying legacy technology on kubernetes?". Well, first of all, I have to say that some technologies that are considered “legacy” have tons of new exciting features for the new workloads and applications. This is, for example, the case of DB2 with spatial queries and many others that you may have a look at (to be honest, I have very few experience with it). Secondly, not all the applications are greenfield. There are lots of old ones that are using those “legacy” technologies. Even some greenfield ones need to use data stored in long life systems. If I want to modernize those applications or build new ones based on these data, I should be able to develop in a friendly environment. And as a developer, I would be more than happy if I can create an instance where I have total control and that I can reuse the agility of containers. That is why I was tempted to try DB2 on containers. I was looking at an alternative to do as quickly as possible, so that is why I took some decisions that maybe do not follow exactly the guidelines that I would link as a reference. DISCLAIMER This is just a personal exercise, it does not represent IBM procedures or recommendations in any way. DB2® is copyright by IBM, every time I refer to DB2 it should be assumed implicitly DB2®. It is not an open source technology under different licences by IBM, in this case I would use the community edition, whose license and other details are commented . I would like to mention that is only an exercise. Please, use the recommended, documented and supported procedure if you need an stable version for your particular use case. Additionally we should remember db2 technology as most of the traditional databases, uses low level access to some resources (like shared memory, processes, and so on) to perform best on bare metal/virtual environments. That means it still , and although they are working on adapting better to cloud and containers, we are not used to see them in container native technologies. PREREQUISITES KUBERNETES As I have done in previous posts, I am using minikube for this exercise and a local domain served by dnsmasq on my laptop. That provides me the ability of working offline once the container images are deployed on the cluster. In my test I have used a configuration or 6 cpus and 12 Gb of RAM, as suggested in . If we are not deploying the Unified Console, we can reduce it to 4 cpus. I used the following command (check your driver): minikube start --cpus=6 --memory=12288 --driver=kvm2 --addons dashboard,ingress,metrics-server Additionally, in some steps we will need some ingress objects with passthrough ssl. That will simplify to provide SNI negotiation. To , review the deployment and add the --enable-ssl-passthrough arguments to the container ones. IBM API KEY You should register in IBM as a developer, and create an API key. I have used the console using . DEPLOYMENT I have modified a couple of things from the documentation I have found and linked, the most relevant are: * Use helm 3 instead of helm 2, because after November 2020 . * In vanilla kubernetes for developers, we don’t need to worry about , which provides an extra security to OpenShift environments. As our dev cluster is in a laptop, I can relax a little bit the security requirements (note: if the environment is shared, review the security or consider alternatives to avoid bad times). PREPARATION First, clone the repo . Go to stable/ibm-db2 folder, where there you can find most of the artifacts and information to deploy db2 community edition on OpenShift. We are now creating the namespace for deploying the database. We are creating a service account and provide the privileges through a role and a role binding to create some additional objects, as the helm chart will deploy some jobs to create the whole infrastructure on the namespace. kubectl create namespace db2 kubectl create serviceaccount db2u -n db2 kubectl create -f ibm_cloud_pak/pak_extensions/pre-install/namespaceAdministration/ibm-db2-role.yaml -n db2 kubectl create -f ibm_cloud_pak/pak_extensions/pre-install/namespaceAdministration/ibm-db2-rb.yaml -n db2 Then, as we will use a private registry (the ibm one), we need to set up the credentials for it. With the API Key that you should have obtained in the prerequisites, create a secret and add it to the service account to allow pulling the images needed for the deployment. kubectl create secret -n db2 docker-registry ibm-registry \ --docker-server=icr.io \ --docker-username=iamapikey \ --docker-password=&lt;api_key&gt; kubectl patch serviceaccount db2u -n db2 -p '{"imagePullSecrets": [{"name": "ibm-registry"}]}' We are now ready to deploy the database. DEPLOYMENT The official deployment script is db2u-install on the ibm_cloud_pak/pak_extensions/common folder. It encapsulates the helm 2 script, so I have done a little hack in order to use helm 3 and avoid using tiller. Also, I wanted to get the secrets created automatically, so I need to change generateSecrets=true that is false in the original script. With all those changes, we can create the jobs that will deploy all the infra, executing the helm command from stable/ibm-db2 folder. helm install db2poc $PWD --namespace db2 --values $PWD/values.yaml --set arch=x86_64 --set servicename=db2poc --set ldap.ldap_server=db2poc-db2u-ldap --set global.dbType=db2oltp --set database.name=BLUDB --set generateSecrets=true --set storage.storageClassName=standard --set storage.useDynamicProvisioning=true And voilá, after downloading images, and creating everything,we have a brand new db2 instance in our kuberenets cluster. We are going to do some additional tests to double check everything is ok. In the following picture, we can see the main architecture for the deployment. CREATING A DEVELOPER USER From , we have some hints on how to create an user on the ldap that this db2 instance uses as identity provider. tools_pod=$(kubectl get po -n db2 -o name | grep db2poc-db2u-tools) kubectl exec -it ${tools_pod} -- addLdapUser.py -u admin -p h4ck1t -r admin We will get something like Next UID will be 5003 Adding admin to LDAP server Updating LDAP password for user admin Added user to LDAP server After creating the user on the ldap, we check it is valid for use as db2 user: kubectl exec -it db2poc-db2u-0 -- id admin kubectl exec -it db2poc-db2u-0 -- su db2inst1 -c "~/sqllib/bin/db2 connect to bludb user admin using h4ck1t" If everything is ok, we will get uid=5003(admin) gid=3000(bluadmin) groups=3000(bluadmin) Database Connection Information Database server = DB2/LINUXX8664 11.5.4.0 SQL authorization ID = ADMIN Local database alias = BLUDB OPTIONAL: UNIFIED CONSOLE The instructions to deploy the unified console in the same namespace as the db2 instance are explaine in . Like previously, we prefer to use helm 3, so instead of using deploy-console.sh script, we are running directly the helm command. From the root path of your github clone, move to the stable/ibm-unified-console folder. Then, run the following command. helm install db2poc-console $PWD --set configMapName=db2poc-db2u-uc-config --set dataServer.ldap.rootPwdSecretName=db2poc-db2u-ldap --set dataServer.metadb.pwdSecretName=db2poc-db2u-instance --set dataServer.sharedPVC.name=db2poc-db2u-sqllib-shared --set global.image.secretName=ibm-registry -f $PWD/ibm_cloud_pak/pak_extensions/values-standalone-console.yaml --namespace db2 After deploying the chart, an additional ingress is needed. The deploy-console.sh creates the route object for OpenShift, a similar command for vanilla kubernetes ingress on nginx whould be: cat &lt;&lt; _EOF_ |kubectl apply -f - apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: db2poc-console namespace: db2 annotations: kubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/ssl-passthrough: "true" spec: rules: - host: db2poc-console.db2.minikube.cloud http: paths: - backend: service: name: db2poc-console-ibm-unified-console-ui port: number: 443 path: / pathType: ImplementationSpecific _EOF_ If everything is right, you may login into the ingress host (in my case, ) with the user credentials you have created previously and see a picture like the following. The whole architecture including the unified console now is as depicted. TESTING The chart deploys a nodeport service to access the database connecting through plain and ssl ports. We are going to do some initial tests based on it, but we are going to set up an ingress to use ssl passthrough to connect from outside the cluster. I am testing the connection using , which is under . These instructions will be similar to other jdbc-based database tools. At the time of writing this post, I have used the 11.5.4 client jdbc driver that can be downloaded from . As a reference, I use the document for setting up the connection. echo "Minikube IP is" $(minikube ip) echo "Non-secure NodePort is" $(kubectl get service db2poc-db2u-engn-svc -o jsonpath='{.spec.ports[?(@.name=="legacy-server")].nodePort}' ) Bearing in mind the default database is bludb, we add the alias jdbc:db2://&lt;minikube_ip&gt;:&lt;nodeport&gt;/bludb to connect, and get a successful session through the unsecure nodeport. I usually prefer to connect through a ssl passthrough ingress to the database from outside. A nodeport is ok, but has some drawbacks. If you want to try the ingress option, it is as simple as creating it with: cat &lt;&lt; _EOF_ |kubectl apply -f - apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: db2poc-db2u namespace: db2 annotations: kubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/ssl-passthrough: "true" spec: rules: - host: "db2poc-db2u.minikube.cloud" http: paths: - backend: service: name: db2poc-db2u port: number: 50001 path: / pathType: ImplementationSpecific _EOF_ Now, you need the cert chain to add to the jdbc properties. You can extract the CA and the certificate connecting with openssl to the secured ingress connection, in my case using the following. echo -n | openssl s_client -connect db2poc-db2u.minikube.cloud:443 -servername db2poc-db2u.minikube.cloud -showcerts 2&gt;/dev/null |sed -ne '/-BEGIN CERTIFICATE-/,/-END CERTIFICATE-/p' &gt; /tmp/certs.pem In order to double check, the cert subject should be: CN=ibm.com,OU=ICP,O=IBM,L=Toronto,ST=Ontario,C=CA CN=ibm.com,OU=db2ssl,O=IBM,L=Toronto,ST=Ontario,C=CA You need to add those certs to a java keystore (jks). We can do it through the command line with keytool or use a visual tool like . Once you have the jks, you only need to add that information to the jdbc. A brief and useful can help you through the parameters. An example of secured url through the ingress is jdbc:db2://db2poc-db2u.minikube.cloud:443/bludb:sslConnection=true;sslTrustStoreLocation=/tmp/db2-server.jks;sslTrustStorePassword=h4ck1t;. Don’t forget the semicolon at the end of the url! MORE INFORMATION Other than the previous links and their related pages, you may find some useful information about the architecture, sizing, etc, for a production deployment on OpenShift in&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/qPr-LeKzOYU" height="1" width="1" alt=""/&gt;</content><dc:creator /><feedburner:origLink>https://blog.ramon-gordillo.dev/2021/03/trying-db2-in-kubernetes-for-developers/</feedburner:origLink></entry><entry><title type="html">Starting business processes using Kafka events</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/lOnWfH1OUk8/starting-business-processes-using-kafka-events.html" /><author><name>Karina Varela</name></author><id>https://blog.kie.org/2021/03/starting-business-processes-using-kafka-events.html</id><updated>2021-03-07T23:30:57Z</updated><content type="html">Let’s check how we can model a business process, using the BPMN standard, that can react to events. Whenever a new event is published in a specific Kafka topic, a new process instance should be started. We’ll also check how to configure the project and environment in order to achieve these goals. GETTING STARTED To get started we should create, deploy and test an event-driven process application.  1. PREPARING YOUR ENVIRONMENT The samples described in this guide were created using the following technologies: * Java 11 * Maven, Git *  7.48+ or  7.10+ * Kafka  INFO: This feature was released in this specific jBPM and RHPAM version. To achieve this post’s goals, you must use the mentioned versions or higher. If you don’t know how to install jBPM locally, take a look at: . 1.1. PREPARING YOUR KAFKA SERVER AND TOPICS Event-driven processes interacts with other services via event platforms, more specifically in our case, Kafka topics. In this application, our process needs interacts with three topics: “incoming-requests“, “requests-approved” and “requests-denied“. Let’s now setup a Kafka environment and create these three topics. We will use Strimzi and docker compose to help us get up and running faster. INFO: This guide focus is not Kafka, therefore the following steps are straightforward. If you need more details about the upcoming commands please refer to this post: . First, clone the project that contains the docker-compose file we’ll use to start the Kafka services. Next start the services. Check the commands below: git clone cd amq-examples/strimzi-all-in-one/ docker-compose up Open a new tab in your terminal, access the cloned project folder (amq-examples/strimzi-all-in-one/) and create the three topics: docker-compose exec kafka bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic incoming-requests docker-compose exec kafka bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic requests-approved docker-compose exec kafka bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic requests-denied Now that we have the three topics ready to be used as our communication layer between services and our process, we can start working on the process definition. THE USE CASE AND THE SHIFT TO AN EVENT-DRIVEN PROCESS APPLICATION Our use case will be the automation of a credit card limit raise approval process. Most card issuers allow customers to request an increased credit limit through their websites, mobile apps or over the phone. Let’s consider we need to deliver this automation for a bank that wants to achieve a similar use case within an event-driven architecture. TIP: We’ll simplify the business logic of this use case to give focus to the eventing features and how you can use it. The existing process is started via REST. It has a step for automatic request validation using DMN, and if the request not approved, it goes to a manual analysis. If approved, the service responsible for updating the cc limit is invoked via REST (the diagram only represents this REST call with a script task since this is not relevant for this guide’s scenario). Finally, the process ends either with an approved or denied request. Image 1: Process v1. The process starts based on a rest call or JAVA API invocation. Services involved in the process are invoked via rest. Now, with the architecture shift, the service responsible for increasing the credit card limit should not be invoked via REST anymore. The external service now listens to the topic “request-approved” in order to track when to execute the limit raise. The business process should get started based on events, and whenever the process finishes, it should post a message to a specific topic depending on whether the request was approved or not. Notice how the process below can achieve the same goals, using an event-driven strategy: Image 2: Process v2. Whenever a new event happens in a topic, a new instance will be triggered. Depending on how this process ends, an event is published in a different topic, therefore, different services can react based on the approval status. In this strategy we have a resilient way of communication between services where the broker is responsible for storing and providing the events. Adding to that, the tech team can evolve the solutions by using the features available in Kafka itself, like the possibility to replay all the events that happened in a specific time, in chronological order. 2. ENABLING THE JBPM (RHPAM) KAFKA EXTENSION To enable Kafka capabilities in the KIE Server (engine) we need to use system properties in the runtime environment. You can enable it both for SpringBoot and WildFly (a.k.a. jBoss) based deployments. See below the command that uses the jboss-cli.sh (or .bat) script to add the system property in WildFly, and, restart it. TIP: When adding new system properties to WildFly or jBoss EAP, it’s necessary to restart it to have the new system properties activated. INFO: There are more options in jBPM to customize the Kafka address, topic names, etc. In our case, we’re using the default Kafka address, which is, localhost:9092. More customization information can be found in the official Red Hat product documentation: . With WildFly or EAP up and running, you can enable the Kafka extension in the KIE Server by executing the commands below: $ $JBOSS_HOME/bin/jboss-cli.sh -c [standalone@localhost:9990 /] /system-property=org.kie.kafka.server.ext.disabled:add(value=false) [standalone@localhost:9990 /] :shutdown(restart=true) We’re now ready to start working on the process definition. 3. STARTING PROCESSES USING EVENTS * First, import the existing project with process v1 in Business Central. * Open the cc-limit-raise-approval process. * The first step is to change the start event to a start message event: Image 3: Convert start event to start message event Whenever a customer do a new request (independently of the channel used) an event should be published on the “new-requests” topic. With that, a new process instance will be started whenever a new event is published in this topic. * Configure the name of the Kafka topic in the starting message event. Image 4: Configure the message with the same name of the topic it will listen to. * We want to receive the request contained in the event data. The engine provides automatic marshalling to help us mapping the input directly to a data object. The project has an object named “LimitRaiseRequest.java” which we will use to receive the incoming data. On the properties panel of the Start Message Event, configure the input data: Image 5: Start message event configuration of the Input data * Save the process. * Now, deploy the project to KIE Server so you can test it. You can use the deploy button available in Business Central. * Open a new tab in the terminal, and access the “” project we’re using to interact with the Kafka service. cd $PROJECTS_DIR/amq-examples/strimzi-all-in-one docker-compose exec kafka bin/kafka-console-producer.sh --topic incoming-requests --bootstrap-server localhost:9092 &gt; The producer service is now waiting for you to publish an event to the topic “incoming-requests“. To do so, simply input the following json data and hit enter: {"data" : {"customerId": 1, "customerScore": 250, "requestedValue":1500}} &gt; {"data" : {"customerId": 1, "customerScore": 250, "requestedValue":1500}} * Now, in your browser, in Business Central, if you go to the Process Instances management page and filter by the Completed status, you should be able to see a process instance completed: Image 6: Business Central. List of completed process instances in the monitored KIE Server. * Select the process instance you see, and next, go the the Diagram tab. You should see that the request was automatically approved. Image 7: Process Instance Diagram. This process instance was started based on an event that happened in the topic configured in the message start event. -------------------------------------------------------------------------------- You can now effectively handle the events that triggers business processes within an event-driven architecture. The next step is to learn how to emit events from within your process. The following post should bring you details on how to let the ecosystem know about key happenings of your business process. -------------------------------------------------------------------------------- This blog post is part of the seventh section of the  series: . The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/lOnWfH1OUk8" height="1" width="1" alt=""/&gt;</content><dc:creator>Karina Varela</dc:creator><feedburner:origLink>https://blog.kie.org/2021/03/starting-business-processes-using-kafka-events.html</feedburner:origLink></entry></feed>
